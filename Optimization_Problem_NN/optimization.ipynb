{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c07f71-f9e9-4f7d-a0fb-dbebee17b184",
   "metadata": {},
   "source": [
    "# Exploratory Analysis On Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79a235c-1d5d-4130-8b99-0c1c6c34cf09",
   "metadata": {},
   "source": [
    "The problem is pretty straightforward. According to the \"A Brief Tour of Deep Learning from a Statistical Perspective\" (Nalisnick et al.) we see that:\n",
    "\n",
    "```\n",
    "While DNNs have been shown to be universal approximators for some time, these results do not guarantee anything about the class of functions that can be reached by SGD. Thus, there has been much interest in studying the optimization landscape of these models. For many years, it was thought that NN optimization would be hopelessly plagued by local minima (Cheng & Titterington 1994). However, this concern has been alleviated, to a degree, with more recent conjectures that it is not local minima but saddle points that comprise many of the loss surfaceâ€™s critical points (Dauphin et al. 2014, Kawaguchi 2016). The intuition is that it is unlikely that the optimization surface will be going the same direction in every dimension, as is necessary to build a local minimum. In consequence, much attention has been given to escaping saddle points efficiently ( Jin et al. 2017). In addition to classifying critical points, the qualities of the minima are also of interest. In particular, whether minima are wide and flat versus narrow and sharp has been of keen interest (Hochreiter & Schmidhuber 1997a, Keskar et al. 2017). The intuition is that wide minima are likely to generalize to never-before-seen data since there is a neighborhood of parameters that represent roughly equivalent solutions.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9af1e25-47bf-42ea-b38d-39a7f0d7b82b",
   "metadata": {},
   "source": [
    "## Setting up our Basic Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9cba7d-d5ff-41d8-96b6-2d53af01a282",
   "metadata": {},
   "source": [
    "Here we setup a basic network using Xavier Initalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b58b9314-219d-4d5e-ba38-91427c480b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f2c6c1d-bbc1-4b84-abe1-b9521eeaa8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objs as go\n",
    "import plotly.offline as pyo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1590722-4f56-4f6b-8963-a41cef690203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95845cbb-023d-494f-9993-30cb3830daf1",
   "metadata": {},
   "source": [
    "### Sequential Class [Harness]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0207025-ad39-40d6-a453-fd17a2776fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(tf.Module):\n",
    "    \"\"\"\n",
    "    Follows a similar format for the keras implementation of Sequential. \n",
    "    Its super basic, but its good for many testing and benchmarking purposes.\n",
    "    \"\"\"\n",
    "    def __init__(self,**kwargs):  \n",
    "        super().__init__(**kwargs)\n",
    "        self._layers = []\n",
    "        self._optimizer = None\n",
    "        self._loss_func = None\n",
    "        self.lr = None # This is a custom learning rate, it won't be used if we use a tensorflow optimizer \n",
    "    \n",
    "    def _compile(self, \n",
    "                 optimizer = None,\n",
    "                 loss_fn = None,\n",
    "                ):\n",
    "        self._optimizer = optimizer\n",
    "        self._loss_func = loss_fn\n",
    "    \n",
    "    def _predict(self, x0): # Forward Propogation\n",
    "        for layer in self._layers:\n",
    "            x0 = layer(x0)\n",
    "        return x0\n",
    "    \n",
    "    @tf.function # Graph-mode Execution (Symbolic differentiation), X | Y are automatically converted to tensors\n",
    "    @tf.autograph.experimental.do_not_convert # Suppress Autograph Conversion\n",
    "    def _train_step(self, x, y):\n",
    "        with tf.GradientTape(persistent=True) as t:\n",
    "                current_loss = self._loss_func(y, self._predict(x))\n",
    "                \n",
    "        gradients = t.gradient(current_loss, self.trainable_variables)\n",
    "        self._optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        # This is for custom optimizers...\n",
    "#         for i_trainable_variable, i_gradient in zip(self.trainable_variables, gradient):\n",
    "#             i_trainable_variable.assign_sub(self.lr * i_gradient)\n",
    "        \n",
    "        return current_loss # Returns loss tensor\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self._layers.append(layer)\n",
    "    \n",
    "    def fit(self,x, y, n_epochs, batch_size):      \n",
    "        # Conversions (assume float32 network)\n",
    "        x = tf.cast(x, tf.float32)\n",
    "        y = tf.cast(y, tf.float32)\n",
    "            \n",
    "        for _epoch in range(n_epochs):\n",
    "            # Generate a shared shuffle index\n",
    "            indices = tf.range(start=0, limit=x.shape[0], dtype=tf.int32)\n",
    "            shuffled_indices = tf.random.shuffle(indices)\n",
    "            \n",
    "            # Shuffle data and labels using the shared shuffle index\n",
    "            shuffled_x = tf.gather(x, shuffled_indices)\n",
    "            shuffled_y = tf.gather(y, shuffled_indices)\n",
    "            \n",
    "            # Batch loop\n",
    "            for step in range(0, shuffled_x.shape[0], batch_size):\n",
    "                batch_x = shuffled_x[step : step + batch_size]\n",
    "                batch_y = shuffled_y[step : step + batch_size]\n",
    "                \n",
    "                loss = self._train_step(batch_x, batch_y)\n",
    "                print(\"Training loss in epoch {0} = {1}\".format(_epoch, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f519fc54-a6b1-413f-929e-6a012254a095",
   "metadata": {},
   "source": [
    "### Dense Layer \n",
    "\n",
    "* Xavier Intialization **by default**\n",
    "* Linear Activation Function **by default**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8855bb99-225d-43c4-a934-72bf4f6b244d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer(tf.Module):\n",
    "    \"\"\"\n",
    "    Regular Dense Layer found in many regular Neural Networks\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size, \n",
    "        output_size, \n",
    "        output_layer=False, \n",
    "        activation_function=\"linear\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.input_size = input_size;\n",
    "        self.output_size = output_size;\n",
    "        self.output_layer = output_layer;\n",
    "        self.activation_function = activation_function;\n",
    "        \n",
    "        # Weight Scheme\n",
    "        self.w = tf.Variable(\n",
    "            tf.random.normal([input_size, output_size]) * tf.sqrt(2 / (input_size + output_size)),\n",
    "            name='w'\n",
    "        );\n",
    "        \n",
    "        # Bias Scheme\n",
    "        self.b = tf.Variable(0.0, name='b');\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        match self.activation_function: # Works with Python 3.10 and above\n",
    "            case \"leaky_relu\":\n",
    "                result = tf.nn.leaky_relu(x @ self.w + self.b)\n",
    "            case \"relu\":\n",
    "                result = tf.nn.relu(x @ self.w + self.b)\n",
    "            case \"softmax\":\n",
    "                result = tf.nn.softmax(x @ self.w + self.b)\n",
    "            case \"sigmoid\":\n",
    "                result = tf.nn.sigmoid(x @ self.w + self.b)\n",
    "            case _:\n",
    "                result = (x @ self.w + self.b) # I believe this is just the linear result\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5ddec-23b3-4635-8eed-4b0a7c4c06a9",
   "metadata": {},
   "source": [
    "## Guassian Clusters on 2D plane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "837b006e-5483-4eb5-8f70-9a8bd727e0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_amnt = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbdeb3c7-b932-47db-81db-563b5fb6ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Easy Linear Seperation\n",
    "cluster_1 = np.random.multivariate_normal([10, 10], [[1, 0], [0, 5]], cluster_amnt)\n",
    "cluster_2 = np.random.multivariate_normal([-10, 10], [[1, 0], [0, 5]], cluster_amnt)\n",
    "cluster_3 = np.random.multivariate_normal([10, -10], [[1, 0], [0, 5]], cluster_amnt)\n",
    "cluster_4 = np.random.multivariate_normal([-10, -10], [[1, 0], [0, 5]], cluster_amnt)\n",
    "\n",
    "# No Clear Linear Seperation\n",
    "# cluster_1 = np.random.multivariate_normal([10, 10], [[1, 0], [0, 30]], cluster_amnt)\n",
    "# cluster_2 = np.random.multivariate_normal([-10, 10], [[1, 0], [0, 30]], cluster_amnt)\n",
    "# cluster_3 = np.random.multivariate_normal([10, -10], [[1, 0], [0, 30]], cluster_amnt)\n",
    "# cluster_4 = np.random.multivariate_normal([-10, -10], [[1, 0], [0, 30]], cluster_amnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78bac912-2353-4de0-8777-cb21f3b94d11",
   "metadata": {},
   "source": [
    "### Pack into a Pandas for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2e4e64d-d05f-48f6-aea5-6f2f08945456",
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = np.empty(cluster_amnt)\n",
    "l2 = np.empty(cluster_amnt)\n",
    "l3 = np.empty(cluster_amnt)\n",
    "l4 = np.empty(cluster_amnt)\n",
    "\n",
    "l1.fill(0)\n",
    "l2.fill(1)\n",
    "l3.fill(2)\n",
    "l4.fill(3)\n",
    "\n",
    "labels = np.concatenate((l1, l2, l3, l4), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b338764-f6cd-4efd-a235-13f91b4a0a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              f1         f2    Y\n",
      "0       9.455424   9.082024  0.0\n",
      "1       9.191688   9.775133  0.0\n",
      "2      11.179970  10.030100  0.0\n",
      "3       8.708052  11.417686  0.0\n",
      "4       9.710806  11.759120  0.0\n",
      "...          ...        ...  ...\n",
      "19995 -10.530834 -12.085361  3.0\n",
      "19996  -9.404026 -10.263830  3.0\n",
      "19997  -9.606129  -8.205653  3.0\n",
      "19998  -8.660109  -9.715495  3.0\n",
      "19999  -8.984113 -10.369897  3.0\n",
      "\n",
      "[20000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "x = np.concatenate((cluster_1.T[0], cluster_2.T[0], cluster_3.T[0], cluster_4.T[0]), axis=0)\n",
    "y = np.concatenate((cluster_1.T[1], cluster_2.T[1], cluster_3.T[1], cluster_4.T[1]), axis=0)\n",
    "\n",
    "data = {\n",
    "    \"f1\": x,\n",
    "    \"f2\": y,\n",
    "    \"Y\": labels\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea60b213-3372-4ea1-9df5-4474928cd80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='f1', y='f2', color='Y', title='Gaissian Clusters')\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6723d3d-52ae-489a-abc9-c34ff248bdae",
   "metadata": {},
   "source": [
    "### Custom Dense Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612025d4-9396-4129-8271-cc2dad638d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Custom_DNN():\n",
    "    model = Sequential() \n",
    "    \n",
    "    model.add(Dense_Layer(2, 64)) # 2 -> 64 node layer\n",
    "    model.add(Dense_Layer(64, 32)) # 64 -> 32 node layer\n",
    "    model.add(Dense_Layer(32, 16)) # 32 -> 16 node layer\n",
    "    model.add(Dense_Layer(16, 4, activation_function=\"softmax\")) # 16 -> 4 Output layer\n",
    "    \n",
    "    model._compile(\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate = 0.001),\n",
    "        loss_fn = tf.keras.losses.CategoricalCrossentropy()\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35891845-4658-4589-99b2-531197f1e363",
   "metadata": {},
   "outputs": [],
   "source": [
    "Custom_DNN = Custom_DNN();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110a1c65-d3bc-4e36-b627-0546f2d0a0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Custom_Classifier.fit(\n",
    "    x = X_train,\n",
    "    y = y_train,\n",
    "    n_epochs = 100,\n",
    "    batch_size = 16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ddae15-f779-40ea-a585-d968c9f8e1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849af24e-48fc-42c4-8f4d-d7ff9fd0575e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a28134d-a1da-41df-bc11-cbd275a6ecb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
