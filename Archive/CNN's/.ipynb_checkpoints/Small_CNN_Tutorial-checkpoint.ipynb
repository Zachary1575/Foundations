{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e172831b-3bb5-4706-ae9d-298be82db3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a29ecfe8-342e-40de-863b-29933c092fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 22:37:18.652029: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3dcb4cb7-c0ae-4b99-b325-0bc4f7ef046b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b8920-ba69-4f8a-bb89-6d9e3103cf5b",
   "metadata": {},
   "source": [
    "# Data Utility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d392e45-7c95-4bd2-af34-270f0ba7b0f7",
   "metadata": {},
   "source": [
    "Here we import the MNIST Dataset (a quite used dataset). We use the tensorflow loader to help us out and abstract all the methods away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57b5084e-b4d3-444b-a5a5-ca46697e153a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c5b84d-1c15-4a91-b3a5-b8d062a994ee",
   "metadata": {},
   "source": [
    "We then normalize the data so we can bound all the data between [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56e64b00-b985-4551-88b0-edc04d2d2add",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype(\"float32\") / 255.0\n",
    "test_images = test_images.astype(\"float32\") / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5505e3e8-c201-48ce-8f76-5ca32ea12a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data Shape: (60000, 28, 28)\n",
      "Train Label Shape: (60000,)\n",
      "Test Data Shape: (10000, 28, 28)\n",
      "Test Label Shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Some metadata on our data\n",
    "print(f\"Train Data Shape: {train_images.shape}\")\n",
    "print(f\"Train Label Shape: {train_labels.shape}\")\n",
    "print(f\"Test Data Shape: {test_images.shape}\")\n",
    "print(f\"Test Label Shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e6ea0-1479-4e5d-a6c3-d06666bb2186",
   "metadata": {},
   "source": [
    "# Dense Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9024ae73-68b0-4a2f-ba11-816da3577a1c",
   "metadata": {},
   "source": [
    "Here I will build a dense layer from scratch. Note, I am not going to put all the checks such as matrix dimension validation etc. However, it helps us see under the hood. All dense networks are motivated by the equation:\n",
    "\n",
    "$$\n",
    "y = Wx + b\n",
    "$$\n",
    "\n",
    "In addition, all neurons are configured with a loose version of Xavier Initilization Scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5dedc2-e97e-4439-a519-cafd77eb0607",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer(tf.Module):\n",
    "    \"\"\"\n",
    "    Regular Dense Layer found in many regular Neural Networks\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size, \n",
    "        output_size, \n",
    "        output_layer=False, \n",
    "        activation_function=\"linear\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.input_size = input_size;\n",
    "        self.output_size = output_size;\n",
    "        self.output_layer = output_layer;\n",
    "        self.activation_function = activation_function;\n",
    "        \n",
    "        # Weight Scheme\n",
    "        self.w = tf.Variable(\n",
    "            tf.random.normal([input_size, output_size]) * tf.sqrt(2 / (input_size + output_size)),\n",
    "            name='w'\n",
    "        );\n",
    "        \n",
    "        # Bias Scheme\n",
    "        self.b = tf.Variable(0.0, name='b');\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        match self.activation_function: # Works with Python 3.10 and above\n",
    "            case \"leaky_relu\":\n",
    "                result = tf.nn.leaky_relu(x @ self.w + self.b)\n",
    "            case \"relu\":\n",
    "                result = tf.nn.relu(x @ self.w + self.b)\n",
    "            case \"softmax\":\n",
    "                result = tf.nn.softmax(x @ self.w + self.b)\n",
    "            case \"sigmoid\":\n",
    "                result = tf.nn.sigmoid(x @ self.w + self.b)\n",
    "            case _:\n",
    "                result = (x @ self.w + self.b) # I believe this is just the linear result\n",
    "                \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e029d2f-da05-42c8-8ced-a2643a7a31b8",
   "metadata": {},
   "source": [
    "# Convolutional Layer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56863c6-490f-4674-8bcd-303b70bef93c",
   "metadata": {},
   "source": [
    "This is the convolutional layer is specified by a kernel that moves across the image and aggregates information. Unlike dense networks, the kernel serves as the weight matrix! Often times, normal convolutional layers go through the process of:\n",
    "\n",
    "$$\n",
    "\\text{Convolution} \\longrightarrow \\text{Non-linear Activation} \\longrightarrow \\text{Pooling layer (optional)}\n",
    "$$\n",
    "\n",
    "For brevity of the notebook, I will not be using a pooling layer (also our task is very simple enough)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13b4fe68-6403-4f45-ab80-d8d3dfb1ffc5",
   "metadata": {},
   "source": [
    "Here we implement a kernel that moves across an image or feature map to extract features. The GIF below depicts such a process:\n",
    "<center>\n",
    "    <img src=\"https://media.giphy.com/media/i4NjAwytgIRDW/giphy.gif\" width=\"500\" height=\"500\" />\n",
    "</center>\n",
    "\n",
    "Since our images are small with dimensions of $28 \\times 28$, we will use the popular $3 \\times 3$ kernel. The kernel filter values will be using Xavier initialization similar to our dense network above. We imitate the figure above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57ff1c01-c25c-4b10-9855-3aebc050b266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[4., 3., 4.],\n",
      "       [2., 4., 3.],\n",
      "       [2., 3., 4.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# Define a 5 x 5 tensor like that above\n",
    "matrix = tf.constant(\n",
    "    [\n",
    "        [1, 1, 1, 0, 0], \n",
    "        [0, 1, 1, 1, 0], \n",
    "        [0, 0, 1, 1, 1],\n",
    "        [0, 0, 1, 1, 0],\n",
    "        [0, 1, 1, 0, 0],\n",
    "    ], dtype=tf.float32)\n",
    "\n",
    "# We define a 3 x 3 kernel\n",
    "kernel = tf.constant(\n",
    "    [\n",
    "        [1, 0, 1], \n",
    "        [0, 1, 0], \n",
    "        [1, 0, 1],\n",
    "    ], dtype=tf.float32)\n",
    "\n",
    "def simple_convolve_operation(matrix, kernel):\n",
    "    \"\"\"\n",
    "    This method assumes no padding and assumes 1 stride for simplicity purposes\n",
    "    Assumes Grayscale or 2-rank tensor, kernel is also 2-rank tensor\n",
    "    Assumes that kernel is smaller than image\n",
    "    Feature-Dim = (Init Size + padding - kernel Size + 1) / Stride -> (Init Size - Kernel-Size + 1)\n",
    "    \"\"\"\n",
    "    row, col = matrix.shape\n",
    "    k_row, k_col = kernel.shape\n",
    "\n",
    "    feature_map = tf.Variable(tf.zeros([row - k_row + 1, col - k_col + 1], dtype=tf.float32)) # Make this Tensor Mutable\n",
    "    \n",
    "    row_max = int(row - k_row) + 1 # 0, 1, 2\n",
    "    col_max = int(col - k_col) + 1 # 0, 1, 2\n",
    "\n",
    "    for r in range(row_max):\n",
    "        for c in range(col_max):\n",
    "            splice_tensor = matrix[r:r+k_row, c:c+k_col]\n",
    "            _y = tf.reduce_sum(splice_tensor * kernel)\n",
    "            feature_map[r, c].assign(_y)\n",
    "            \n",
    "    return feature_map\n",
    "\n",
    "feature_map_1 = simple_convolve_operation(matrix, kernel)\n",
    "print(feature_map_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc074302-42a4-476f-83c3-094aeec1a363",
   "metadata": {},
   "source": [
    "However, what if we want to convolve over a stack of feature maps? Say we have a $5 \\times 5 \\times 1$ feature map. We convolve it to get 32 (we specify this) feature maps. Now we want to get 64 feature maps from the $5 \\times 5 \\times 32$ image. We also need to include the number of channels now. We can reimagine the following kernel and tensor (not matrix as its stacked) as:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1 & 0 & 0 \\\\ \n",
    "0 & 1 & 1 & 0 & 0 \\\\ \n",
    "0 & 0 & 1 & 1 & 1 \\\\ \n",
    "0 & 0 & 1 & 1 & 0 \\\\ \n",
    "0 & 1 & 1 & 0 & 0 \\\\ \n",
    "\\end{bmatrix}\n",
    "\\times 32 \\quad \\text{- Stacked ontop of each other}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 1 \\\\ \n",
    "0 & 1 & 0 \\\\ \n",
    "1 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "\\times 32 \\quad \\text{- Stacked ontop of each other}\n",
    "$$\n",
    "\n",
    "\n",
    "We can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "58a39690-6070-407e-93b2-560652c34c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Size: (5, 5, 32)\n",
      "Kernel Size: (3, 3, 32)\n",
      "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[128.,  96., 128.],\n",
      "       [ 64., 128.,  96.],\n",
      "       [ 64.,  96., 128.]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "tensor_depth_32 = tf.stack([matrix] * 32, axis = -1)\n",
    "kernel_depth_32 = tf.stack([kernel] * 32, axis = -1)\n",
    "\n",
    "print(f\"Tensor Size: {tensor_depth_32.shape}\")\n",
    "print(f\"Kernel Size: {kernel_depth_32.shape}\")\n",
    "\n",
    "def convolve_depthwise(matrix, kernel):\n",
    "    \"\"\"\n",
    "    Convolve a stack of feature maps depthwise with a kernel.\n",
    "    This method assumes no padding and assumes 1 stride for simplicity purposes.\n",
    "    Assumes matrix is 3-rank tensor (height, width, depth), kernel is 3-rank tensor (height, width, depth).\n",
    "    Assumes that kernel is smaller than matrix in terms of height and width.\n",
    "    \"\"\"\n",
    "    height, width, depth = matrix.shape\n",
    "    k_height, k_width, k_depth = kernel.shape\n",
    "\n",
    "    # Ensure the depth of the kernel matches the depth of the matrix\n",
    "    assert depth == k_depth\n",
    "\n",
    "    # Calculate output dimensions\n",
    "    output_height = height - k_height + 1\n",
    "    output_width = width - k_width + 1\n",
    "\n",
    "    feature_map = tf.Variable(tf.zeros([output_height, output_width], dtype=tf.float32)) # Make this Tensor Mutable\n",
    "\n",
    "    for h in range(output_height):\n",
    "        for w in range(output_width):\n",
    "            splice_tensor = matrix[h:h+k_height, w:w+k_width, :]  # Extract sub-tensor for convolution\n",
    "            # Element-wise multiply the sub-tensor with the kernel, then sum across all channels and spatial dimensions\n",
    "            _y = tf.reduce_sum(splice_tensor * kernel)\n",
    "            feature_map[h, w].assign(_y)\n",
    "\n",
    "    return feature_map\n",
    "\n",
    "    \n",
    "feature_map_stacked = convolve_depthwise(tensor_depth_32, kernel_depth_32)\n",
    "print(feature_map_stacked)\n",
    "\n",
    "# Noticed that 4 * 32 = 128 as its stacked!\n",
    "# Noticed that 3 * 32 = 128 as its stacked!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6e50a1-306f-40b0-97db-491e68da3a98",
   "metadata": {},
   "source": [
    "After convolving over the stacked feature maps. We need to do this 64 times with 64 unique kernels to get 64 feature maps!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "240cbcbb-3c21-4441-a209-5dc98f3c68db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]\n",
      "\n",
      " [[128.  96. 128.]\n",
      "  [ 64. 128.  96.]\n",
      "  [ 64.  96. 128.]]], shape=(64, 3, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tensors_list = []\n",
    "\n",
    "for filter_times in range(64):\n",
    "    tensors_list.append(convolve_depthwise(tensor_depth_32, kernel_depth_32))\n",
    "    \n",
    "stacked_tensors_64 = tf.stack(tensors_list)\n",
    "print(stacked_tensors_64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8915126-ca86-4949-b92c-8efdcccae325",
   "metadata": {},
   "source": [
    "# Writing the Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e2223-fc3a-475d-ae21-4632a8f3ac47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5474c8e9-dc7a-454f-a474-cde7bd8cfc88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adce7a32-4b2d-43b9-90c7-b0f6239a2494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63afc105-b6e6-4270-84b4-070cdd678cff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fc16ef-eb96-4c95-a810-151820fc7c1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984238e2-9e52-4943-9e24-33e3c9c33760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a6c11e-763e-4297-8b54-5bdae9c801d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
