{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0b2d60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Funcs\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as ImagePIL\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from numpy.random import randint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fc06282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "from scipy import stats\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c22c6b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 17:46:45.337474: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "567ec817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Funcs\n",
    "from Unpack_Scaffold_Data import readAndOutputDataset, curveVisualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9bb2ac",
   "metadata": {},
   "source": [
    "# Data Read Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e9157b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_path = \"/Users/zacharyg/Documents/GitHub/fundemental-neural-nets/GANS/Scaffold_GAN/scaffold_dataset_WU_LAB/Prints\"\n",
    "modulus_path = \"/Users/zacharyg/Documents/GitHub/fundemental-neural-nets/GANS/Scaffold_GAN/scaffold_dataset_WU_LAB/Prints/modulus_data_types.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cdd75dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC COUNT: 675\n",
      "Operation Finished.\n",
      "\n",
      "     Index     Modulus  Spacing  Infill  Height  Speed  Temperature   Mass  \\\n",
      "0        1  358.528888      0.8       1     0.1     30          190  0.394   \n",
      "1        2  301.639039      0.9       1     0.1     30          190  0.334   \n",
      "2        3  292.501492      1.0       1     0.1     30          190  0.308   \n",
      "3        4  258.539802      1.1       1     0.1     30          190  0.286   \n",
      "4        5  238.213024      1.2       1     0.1     30          190  0.259   \n",
      "..     ...         ...      ...     ...     ...    ...          ...    ...   \n",
      "670    671  151.559731      0.8       3     0.2     50          230  0.428   \n",
      "671    672   85.074096      0.9       3     0.2     50          230  0.341   \n",
      "672    673   52.285252      1.0       3     0.2     50          230  0.290   \n",
      "673    674   70.811230      1.1       3     0.2     50          230  0.292   \n",
      "674    675   36.627466      1.2       3     0.2     50          230  0.244   \n",
      "\n",
      "     Porosity    Type  \n",
      "0      0.6848    Line  \n",
      "1      0.7328    Line  \n",
      "2      0.7536    Line  \n",
      "3      0.7712    Line  \n",
      "4      0.7928    Line  \n",
      "..        ...     ...  \n",
      "670    0.6576  Gyroid  \n",
      "671    0.7272  Gyroid  \n",
      "672    0.7680  Gyroid  \n",
      "673    0.7664  Gyroid  \n",
      "674    0.8048  Gyroid  \n",
      "\n",
      "[675 rows x 10 columns]\n",
      "Operation Finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, y_df, file_order = readAndOutputDataset(curve_path, modulus_path, reverse=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10c5bef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X SHAPE: (675, 2, 1803)\n",
      "y SHAPE: (675, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "print(\"X SHAPE:\", X.shape);\n",
    "print(\"y SHAPE:\", y.shape);\n",
    "print();\n",
    "\n",
    "\n",
    "# Visualization\n",
    "# curveVisualization(X, y, file_order);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bab23cc",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "542a40cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposeStressData(X_Data):\n",
    "    X = [];\n",
    "    \n",
    "    for data in X_Data:\n",
    "        X.append(data.T);\n",
    "        \n",
    "    return np.array(X);\n",
    "\n",
    "def normalizeStressStrain(x):\n",
    "    for curve_index in range(len(x)):\n",
    "        curve = x[curve_index];\n",
    "        \n",
    "        max_stress_val = np.max(curve[0]);\n",
    "        max_strain_val = np.max(curve[1]);\n",
    "        \n",
    "        curve[0] = curve[0] / max_stress_val;\n",
    "        curve[1] = curve[1] / max_strain_val;\n",
    "        \n",
    "    return x;\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    \n",
    "    Parameters\n",
    "    -----------------\n",
    "    x: Array of Homogenous (RGB) values of input data \n",
    "    \n",
    "    Returns\n",
    "    -----------------\n",
    "    new_imgs: (numpy integer array) Numpy array of normalized data\n",
    "    \"\"\"\n",
    "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "def stringtoCategorical(y):    \n",
    "    data = [];\n",
    "    \n",
    "    for type_index in range(len(y)):\n",
    "        wrd = y[type_index];\n",
    "        encoding = 0.0;\n",
    "        \n",
    "        if (wrd == \"Cubic\"):\n",
    "            encoding = 1.0;\n",
    "        elif (wrd == \"Gyroid\"):\n",
    "            encoding = 2.0;\n",
    "            \n",
    "        data.append([encoding]);\n",
    "        \n",
    "    return np.array(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1c8efc",
   "metadata": {},
   "source": [
    "# Process Parameter Stripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a1434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameterStrip(y):\n",
    "    y_t = y.T;\n",
    "    \n",
    "    Index = y_t[0];\n",
    "    Modulus = y_t[1];\n",
    "    Spacing = y_t[2];\n",
    "    Infill = y_t[3];\n",
    "    Height = y_t[4];\n",
    "    Speed = y_t[5];\n",
    "    Temp = y_t[6];\n",
    "    Mass = y_t[7];\n",
    "    Porosity = y_t[8];\n",
    "    Type = y_t[9];\n",
    "    return Index, Modulus, Spacing, Infill, Height, Speed, Temp, Mass, Porosity, Type\n",
    "\n",
    "Index, Modulus, Spacing, Infill, Height, Speed, Temp, Mass, Porosity, Type = parameterStrip(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd96c627",
   "metadata": {},
   "source": [
    "# Energy Absorption Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62ae65bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675,)\n"
     ]
    }
   ],
   "source": [
    "Energy_Absorption = [];\n",
    "\n",
    "for curve in X:\n",
    "    interval_x = curve[0];\n",
    "    interval_y = curve[1];\n",
    "    \n",
    "    val = integrate.simpson(interval_y, interval_x);\n",
    "    Energy_Absorption.append(val);\n",
    "    \n",
    "Energy_Absorption = np.array(Energy_Absorption);\n",
    "\n",
    "# Sanity Check\n",
    "print(Energy_Absorption.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d684ab9",
   "metadata": {},
   "source": [
    "# Classifcation Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2d80eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_params_1 = y[:, 1:2];\n",
    "cut_params_2 = y[:, 8:9];\n",
    "\n",
    "C_n = np.concatenate((cut_params_1, cut_params_2, (np.reshape(Energy_Absorption, (675,1)))), axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "abbee6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675, 3)\n"
     ]
    }
   ],
   "source": [
    "def integer_encoding(y):\n",
    "    integer_encoding = [];\n",
    "    \n",
    "    for i in y:\n",
    "        integer_encoding.append(i - 1);\n",
    "    \n",
    "    return np.array(integer_encoding);\n",
    "\n",
    "y_label_categorical = integer_encoding(Infill);\n",
    "\n",
    "y_label_categorical = tf.keras.utils.to_categorical(\n",
    "    y_label_categorical, num_classes=3, dtype='float32'\n",
    ");\n",
    "\n",
    "print(y_label_categorical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c46ed17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape: (455, 3)\n",
      "y_train Shape: (455, 3)\n",
      "X_val Shape: (152, 3)\n",
      "y_val Shape: (152, 3)\n",
      "X_test Shape: (68, 3)\n",
      "y_test Shape: (68, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(C_n, y_label_categorical, test_size=0.1, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "\n",
    "print(\"X_train Shape:\", X_train.shape);\n",
    "print(\"y_train Shape:\", y_train.shape);\n",
    "print(\"X_val Shape:\", X_val.shape);\n",
    "print(\"y_val Shape:\", y_val.shape);\n",
    "print(\"X_test Shape:\", X_test.shape);\n",
    "print(\"y_test Shape:\", y_test.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a33c8a",
   "metadata": {},
   "source": [
    "## FC Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133b2d57",
   "metadata": {},
   "source": [
    "$$\n",
    "c_n = \\{Modulus, Infill Density, Energy Absorption \\}\n",
    "$$\n",
    "\n",
    "3 Inputs, 2 outputs, thus our weight tensor will be:\n",
    "$$\n",
    "(3,3)\n",
    "$$\n",
    "\n",
    "Note that tf.Variable() is a wrapper that allows us to do some fancy tensor math!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "417ae31a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-30 17:46:52.212572: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "weights_random = tf.Variable(tf.zeros([3,3])); # sets all the weights to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75b4ce",
   "metadata": {},
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8b3614",
   "metadata": {},
   "source": [
    "Categorical Cross Entropy Loss:\n",
    "\n",
    "Let $n$, be the output size (negative as we are trying to minimize it):\n",
    "$$\n",
    "L_{CE} = - \\sum_{i=1}^{n} y_i \\cdot \\log \\hat{y}_i\n",
    "$$\n",
    "\n",
    "```tf.reduce_sum``` = Computes the sum of elements across dimensions of a tensor, in this case axis is 0.\n",
    "\n",
    "```tf.reduce_mean``` = Computes the mean of elements across dimensions of a tensor, in this case axis is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b5f62d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_crsntrpy(\n",
    "    target_y, \n",
    "    predicted_y):\n",
    "    \"\"\"\n",
    "    Regular Cross Entropy Function\n",
    "    \"\"\"\n",
    "    return - tf.reduce_sum(\n",
    "        tf.reduce_mean(\n",
    "            target_y * tf.math.log(predicted_y + 1e-12), \n",
    "            axis=0\n",
    "        )\n",
    "    );\n",
    "\n",
    "# TF Version\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c604eb94",
   "metadata": {},
   "source": [
    "### Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49497746",
   "metadata": {},
   "source": [
    "```tf.Module``` ~ Helps us handle stuff with tf.Variable. Theoretically, we just need tf.Variable and a Loss to do neural networks. However, thats too much work...\n",
    "\n",
    "Let softmax denote as $\\phi$.\n",
    "\n",
    "```**kargs``` ~ Python passes variable length non keyword argument to function using *args but we cannot use this to pass keyword argument. For this problem Python has got a solution called **kwargs, it allows us to pass the variable length of keyword arguments to the function.\n",
    "\n",
    "We use uniformly randomly initalize the weights using:\n",
    "$$\\alpha = \\mathcal{N}(0, 1) - Tensor$$\n",
    "\n",
    "$$\\alpha \\cdot \\sqrt{\\dfrac{2}{in + out}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6a1c8323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Layer(tf.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        input_size, \n",
    "        output_size, \n",
    "        output_layer=False, \n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.output_layer = output_layer;\n",
    "        \n",
    "        # Weight Scheme\n",
    "        self.w = tf.Variable(\n",
    "            tf.random.normal([input_size, output_size]) * tf.sqrt(2 / (input_size + output_size)),\n",
    "            name='w'\n",
    "        );\n",
    "        \n",
    "        # Bias Scheme\n",
    "        self.b = tf.Variable(0.0, name='b');\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        if self.output_layer:\n",
    "            result = tf.nn.softmax(x @ self.w + self.b)\n",
    "        else:\n",
    "            result = tf.nn.relu(x @ self.w + self.b)\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e48d0f",
   "metadata": {},
   "source": [
    "### Model Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d868356",
   "metadata": {},
   "source": [
    "```with``` ~ The with statement in Python is used for resource management and exception handling. You’d most likely find it when working with file streams. For example, the statement ensures that the file stream process doesn’t block other processes if an exception is raised, but terminates properly.\n",
    "\n",
    "```tf.GradientTape()``` ~ Record operations for automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f198ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC_Classifier(tf.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._layers = [];\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self._layers.append(layer);\n",
    "        \n",
    "    def predict(self, x0):\n",
    "        for _layer in self._layers:\n",
    "            x0 = _layer(x0)\n",
    "        return x0;\n",
    "\n",
    "\n",
    "    def fit(self,\n",
    "            x,\n",
    "            y,\n",
    "            learning_rate,\n",
    "            n_epochs\n",
    "           ):\n",
    "        for _epoch in range(n_epochs):\n",
    "            with tf.GradientTape(persistent=True) as t:\n",
    "    #             current_loss = loss_crsntrpy(y, self.predict(x))\n",
    "                current_loss = cce(y, self.predict(x))\n",
    "\n",
    "            gradients = t.gradient(\n",
    "                current_loss, \n",
    "                self.trainable_variables\n",
    "            )\n",
    "            \n",
    "            # Use an Adam Optimizer\n",
    "            opt = tf.keras.optimizers.experimental.Adam(learning_rate=learning_rate);\n",
    "            opt.apply_gradients(zip(gradients, self.trainable_variables));\n",
    "\n",
    "#             for _trainable_variable, _gradient in zip(self.trainable_variables, gradient):\n",
    "#                 _trainable_variable.assign_sub(learning_rate * _gradient) # Apply Learning rate\n",
    "\n",
    "            train_loss = cce(y, model.predict(x))\n",
    "            test_loss = cce(y, model.predict(x))\n",
    "\n",
    "            print(\"Training loss in epoch {0} = {1}.   Test loss = {2}\".format(_epoch, train_loss.numpy(), test_loss.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938128c",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "6ad653e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier_Sequential():\n",
    "    model = FC_Classifier()\n",
    "    \n",
    "    model.add(FC_Layer(3, 3))\n",
    "    model.add(FC_Layer(3, 3, output_layer=True))\n",
    "    \n",
    "    return model;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a8cea9",
   "metadata": {},
   "source": [
    "### The actual Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3ddd833b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier_Sequential();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "c1aa97a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 200;\n",
    "learning_rate = 0.001;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "888bb8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss in epoch 0 = 2.8046443462371826.   Test loss = 2.8046443462371826\n",
      "Training loss in epoch 1 = 2.7596592903137207.   Test loss = 2.7596592903137207\n",
      "Training loss in epoch 2 = 2.719176769256592.   Test loss = 2.719176769256592\n",
      "Training loss in epoch 3 = 2.6741862297058105.   Test loss = 2.6741862297058105\n",
      "Training loss in epoch 4 = 2.6313822269439697.   Test loss = 2.6313822269439697\n",
      "Training loss in epoch 5 = 2.592966318130493.   Test loss = 2.592966318130493\n",
      "Training loss in epoch 6 = 2.5495896339416504.   Test loss = 2.5495896339416504\n",
      "Training loss in epoch 7 = 2.510291576385498.   Test loss = 2.510291576385498\n",
      "Training loss in epoch 8 = 2.4732022285461426.   Test loss = 2.4732022285461426\n",
      "Training loss in epoch 9 = 2.4324402809143066.   Test loss = 2.4324402809143066\n",
      "Training loss in epoch 10 = 2.396097421646118.   Test loss = 2.396097421646118\n",
      "Training loss in epoch 11 = 2.360969066619873.   Test loss = 2.360969066619873\n",
      "Training loss in epoch 12 = 2.322248935699463.   Test loss = 2.322248935699463\n",
      "Training loss in epoch 13 = 2.2853658199310303.   Test loss = 2.2853658199310303\n",
      "Training loss in epoch 14 = 2.2505271434783936.   Test loss = 2.2505271434783936\n",
      "Training loss in epoch 15 = 2.2187793254852295.   Test loss = 2.2187793254852295\n",
      "Training loss in epoch 16 = 2.184464454650879.   Test loss = 2.184464454650879\n",
      "Training loss in epoch 17 = 2.152597665786743.   Test loss = 2.152597665786743\n",
      "Training loss in epoch 18 = 2.1233580112457275.   Test loss = 2.1233580112457275\n",
      "Training loss in epoch 19 = 2.0909948348999023.   Test loss = 2.0909948348999023\n",
      "Training loss in epoch 20 = 2.0623464584350586.   Test loss = 2.0623464584350586\n",
      "Training loss in epoch 21 = 2.035374879837036.   Test loss = 2.035374879837036\n",
      "Training loss in epoch 22 = 2.0053844451904297.   Test loss = 2.0053844451904297\n",
      "Training loss in epoch 23 = 1.9792137145996094.   Test loss = 1.9792137145996094\n",
      "Training loss in epoch 24 = 1.9545294046401978.   Test loss = 1.9545294046401978\n",
      "Training loss in epoch 25 = 1.9267094135284424.   Test loss = 1.9267094135284424\n",
      "Training loss in epoch 26 = 1.9002628326416016.   Test loss = 1.9002628326416016\n",
      "Training loss in epoch 27 = 1.875933289527893.   Test loss = 1.875933289527893\n",
      "Training loss in epoch 28 = 1.854016900062561.   Test loss = 1.854016900062561\n",
      "Training loss in epoch 29 = 1.8300007581710815.   Test loss = 1.8300007581710815\n",
      "Training loss in epoch 30 = 1.8119852542877197.   Test loss = 1.8119852542877197\n",
      "Training loss in epoch 31 = 1.7889090776443481.   Test loss = 1.7889090776443481\n",
      "Training loss in epoch 32 = 1.767116665840149.   Test loss = 1.767116665840149\n",
      "Training loss in epoch 33 = 1.744829535484314.   Test loss = 1.744829535484314\n",
      "Training loss in epoch 34 = 1.7235910892486572.   Test loss = 1.7235910892486572\n",
      "Training loss in epoch 35 = 1.702691674232483.   Test loss = 1.702691674232483\n",
      "Training loss in epoch 36 = 1.6819782257080078.   Test loss = 1.6819782257080078\n",
      "Training loss in epoch 37 = 1.6637945175170898.   Test loss = 1.6637945175170898\n",
      "Training loss in epoch 38 = 1.6421657800674438.   Test loss = 1.6421657800674438\n",
      "Training loss in epoch 39 = 1.6220216751098633.   Test loss = 1.6220216751098633\n",
      "Training loss in epoch 40 = 1.603379487991333.   Test loss = 1.603379487991333\n",
      "Training loss in epoch 41 = 1.5852347612380981.   Test loss = 1.5852347612380981\n",
      "Training loss in epoch 42 = 1.5694139003753662.   Test loss = 1.5694139003753662\n",
      "Training loss in epoch 43 = 1.5511260032653809.   Test loss = 1.5511260032653809\n",
      "Training loss in epoch 44 = 1.533950924873352.   Test loss = 1.533950924873352\n",
      "Training loss in epoch 45 = 1.5175063610076904.   Test loss = 1.5175063610076904\n",
      "Training loss in epoch 46 = 1.5003010034561157.   Test loss = 1.5003010034561157\n",
      "Training loss in epoch 47 = 1.4837863445281982.   Test loss = 1.4837863445281982\n",
      "Training loss in epoch 48 = 1.4641327857971191.   Test loss = 1.4641327857971191\n",
      "Training loss in epoch 49 = 1.4483753442764282.   Test loss = 1.4483753442764282\n",
      "Training loss in epoch 50 = 1.4388946294784546.   Test loss = 1.4388946294784546\n",
      "Training loss in epoch 51 = 1.4190754890441895.   Test loss = 1.4190754890441895\n",
      "Training loss in epoch 52 = 1.4033244848251343.   Test loss = 1.4033244848251343\n",
      "Training loss in epoch 53 = 1.3984448909759521.   Test loss = 1.3984448909759521\n",
      "Training loss in epoch 54 = 1.3777992725372314.   Test loss = 1.3777992725372314\n",
      "Training loss in epoch 55 = 1.3624409437179565.   Test loss = 1.3624409437179565\n",
      "Training loss in epoch 56 = 1.346052885055542.   Test loss = 1.346052885055542\n",
      "Training loss in epoch 57 = 1.330726981163025.   Test loss = 1.330726981163025\n",
      "Training loss in epoch 58 = 1.3237922191619873.   Test loss = 1.3237922191619873\n",
      "Training loss in epoch 59 = 1.3059418201446533.   Test loss = 1.3059418201446533\n",
      "Training loss in epoch 60 = 1.2928497791290283.   Test loss = 1.2928497791290283\n",
      "Training loss in epoch 61 = 1.2884663343429565.   Test loss = 1.2884663343429565\n",
      "Training loss in epoch 62 = 1.2718398571014404.   Test loss = 1.2718398571014404\n",
      "Training loss in epoch 63 = 1.2609974145889282.   Test loss = 1.2609974145889282\n",
      "Training loss in epoch 64 = 1.2565734386444092.   Test loss = 1.2565734386444092\n",
      "Training loss in epoch 65 = 1.243022084236145.   Test loss = 1.243022084236145\n",
      "Training loss in epoch 66 = 1.2340717315673828.   Test loss = 1.2340717315673828\n",
      "Training loss in epoch 67 = 1.2213863134384155.   Test loss = 1.2213863134384155\n",
      "Training loss in epoch 68 = 1.202034831047058.   Test loss = 1.202034831047058\n",
      "Training loss in epoch 69 = 1.1776736974716187.   Test loss = 1.1776736974716187\n",
      "Training loss in epoch 70 = 1.1534905433654785.   Test loss = 1.1534905433654785\n",
      "Training loss in epoch 71 = 1.1377650499343872.   Test loss = 1.1377650499343872\n",
      "Training loss in epoch 72 = 1.1269527673721313.   Test loss = 1.1269527673721313\n",
      "Training loss in epoch 73 = 1.113703966140747.   Test loss = 1.113703966140747\n",
      "Training loss in epoch 74 = 1.0996965169906616.   Test loss = 1.0996965169906616\n",
      "Training loss in epoch 75 = 1.075829267501831.   Test loss = 1.075829267501831\n",
      "Training loss in epoch 76 = 1.058756947517395.   Test loss = 1.058756947517395\n",
      "Training loss in epoch 77 = 1.045614242553711.   Test loss = 1.045614242553711\n",
      "Training loss in epoch 78 = 1.0375295877456665.   Test loss = 1.0375295877456665\n",
      "Training loss in epoch 79 = 1.0226541757583618.   Test loss = 1.0226541757583618\n",
      "Training loss in epoch 80 = 1.0143789052963257.   Test loss = 1.0143789052963257\n",
      "Training loss in epoch 81 = 1.0081895589828491.   Test loss = 1.0081895589828491\n",
      "Training loss in epoch 82 = 0.998161792755127.   Test loss = 0.998161792755127\n",
      "Training loss in epoch 83 = 0.9988102316856384.   Test loss = 0.9988102316856384\n",
      "Training loss in epoch 84 = 0.988371729850769.   Test loss = 0.988371729850769\n",
      "Training loss in epoch 85 = 0.9827465415000916.   Test loss = 0.9827465415000916\n",
      "Training loss in epoch 86 = 0.974075198173523.   Test loss = 0.974075198173523\n",
      "Training loss in epoch 87 = 0.9663310647010803.   Test loss = 0.9663310647010803\n",
      "Training loss in epoch 88 = 0.9651322364807129.   Test loss = 0.9651322364807129\n",
      "Training loss in epoch 89 = 0.9652312397956848.   Test loss = 0.9652312397956848\n",
      "Training loss in epoch 90 = 0.9642004370689392.   Test loss = 0.9642004370689392\n",
      "Training loss in epoch 91 = 0.9641363024711609.   Test loss = 0.9641363024711609\n",
      "Training loss in epoch 92 = 0.9632725119590759.   Test loss = 0.9632725119590759\n",
      "Training loss in epoch 93 = 0.9630459547042847.   Test loss = 0.9630459547042847\n",
      "Training loss in epoch 94 = 0.962348461151123.   Test loss = 0.962348461151123\n",
      "Training loss in epoch 95 = 0.96196049451828.   Test loss = 0.96196049451828\n",
      "Training loss in epoch 96 = 0.9614282250404358.   Test loss = 0.9614282250404358\n",
      "Training loss in epoch 97 = 0.9608798027038574.   Test loss = 0.9608798027038574\n",
      "Training loss in epoch 98 = 0.9605118632316589.   Test loss = 0.9605118632316589\n",
      "Training loss in epoch 99 = 0.9598040580749512.   Test loss = 0.9598040580749512\n",
      "Training loss in epoch 100 = 0.9595993757247925.   Test loss = 0.9595993757247925\n",
      "Training loss in epoch 101 = 0.9587334394454956.   Test loss = 0.9587334394454956\n",
      "Training loss in epoch 102 = 0.9571489095687866.   Test loss = 0.9571489095687866\n",
      "Training loss in epoch 103 = 0.9577823281288147.   Test loss = 0.9577823281288147\n",
      "Training loss in epoch 104 = 0.9610871076583862.   Test loss = 0.9610871076583862\n",
      "Training loss in epoch 105 = 0.9567723274230957.   Test loss = 0.9567723274230957\n",
      "Training loss in epoch 106 = 0.9603055715560913.   Test loss = 0.9603055715560913\n",
      "Training loss in epoch 107 = 0.9557662010192871.   Test loss = 0.9557662010192871\n",
      "Training loss in epoch 108 = 0.9595295786857605.   Test loss = 0.9595295786857605\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss in epoch 109 = 0.954763650894165.   Test loss = 0.954763650894165\n",
      "Training loss in epoch 110 = 0.9587594270706177.   Test loss = 0.9587594270706177\n",
      "Training loss in epoch 111 = 0.9537649154663086.   Test loss = 0.9537649154663086\n",
      "Training loss in epoch 112 = 0.9579948782920837.   Test loss = 0.9579948782920837\n",
      "Training loss in epoch 113 = 0.9527699947357178.   Test loss = 0.9527699947357178\n",
      "Training loss in epoch 114 = 0.9572362303733826.   Test loss = 0.9572362303733826\n",
      "Training loss in epoch 115 = 0.9517788887023926.   Test loss = 0.9517788887023926\n",
      "Training loss in epoch 116 = 0.9564834833145142.   Test loss = 0.9564834833145142\n",
      "Training loss in epoch 117 = 0.9507916569709778.   Test loss = 0.9507916569709778\n",
      "Training loss in epoch 118 = 0.9557368159294128.   Test loss = 0.9557368159294128\n",
      "Training loss in epoch 119 = 0.9498082995414734.   Test loss = 0.9498082995414734\n",
      "Training loss in epoch 120 = 0.9549962282180786.   Test loss = 0.9549962282180786\n",
      "Training loss in epoch 121 = 0.9488288164138794.   Test loss = 0.9488288164138794\n",
      "Training loss in epoch 122 = 0.9542617797851562.   Test loss = 0.9542617797851562\n",
      "Training loss in epoch 123 = 0.9478533267974854.   Test loss = 0.9478533267974854\n",
      "Training loss in epoch 124 = 0.9535337090492249.   Test loss = 0.9535337090492249\n",
      "Training loss in epoch 125 = 0.946881890296936.   Test loss = 0.946881890296936\n",
      "Training loss in epoch 126 = 0.9528120160102844.   Test loss = 0.9528120160102844\n",
      "Training loss in epoch 127 = 0.9459143877029419.   Test loss = 0.9459143877029419\n",
      "Training loss in epoch 128 = 0.9520966410636902.   Test loss = 0.9520966410636902\n",
      "Training loss in epoch 129 = 0.944951057434082.   Test loss = 0.944951057434082\n",
      "Training loss in epoch 130 = 0.9513880014419556.   Test loss = 0.9513880014419556\n",
      "Training loss in epoch 131 = 0.9439917802810669.   Test loss = 0.9439917802810669\n",
      "Training loss in epoch 132 = 0.9506859183311462.   Test loss = 0.9506859183311462\n",
      "Training loss in epoch 133 = 0.943036675453186.   Test loss = 0.943036675453186\n",
      "Training loss in epoch 134 = 0.9499906897544861.   Test loss = 0.9499906897544861\n",
      "Training loss in epoch 135 = 0.9420857429504395.   Test loss = 0.9420857429504395\n",
      "Training loss in epoch 136 = 0.9493021368980408.   Test loss = 0.9493021368980408\n",
      "Training loss in epoch 137 = 0.9411390423774719.   Test loss = 0.9411390423774719\n",
      "Training loss in epoch 138 = 0.9486204981803894.   Test loss = 0.9486204981803894\n",
      "Training loss in epoch 139 = 0.9401966333389282.   Test loss = 0.9401966333389282\n",
      "Training loss in epoch 140 = 0.9479458928108215.   Test loss = 0.9479458928108215\n",
      "Training loss in epoch 141 = 0.9392584562301636.   Test loss = 0.9392584562301636\n",
      "Training loss in epoch 142 = 0.9472780823707581.   Test loss = 0.9472780823707581\n",
      "Training loss in epoch 143 = 0.9383246898651123.   Test loss = 0.9383246898651123\n",
      "Training loss in epoch 144 = 0.9425585269927979.   Test loss = 0.9425585269927979\n",
      "Training loss in epoch 145 = 0.938744306564331.   Test loss = 0.938744306564331\n",
      "Training loss in epoch 146 = 0.9417986273765564.   Test loss = 0.9417986273765564\n",
      "Training loss in epoch 147 = 0.9376226663589478.   Test loss = 0.9376226663589478\n",
      "Training loss in epoch 148 = 0.9410497546195984.   Test loss = 0.9410497546195984\n",
      "Training loss in epoch 149 = 0.9366260170936584.   Test loss = 0.9366260170936584\n",
      "Training loss in epoch 150 = 0.9403080344200134.   Test loss = 0.9403080344200134\n",
      "Training loss in epoch 151 = 0.9356337189674377.   Test loss = 0.9356337189674377\n",
      "Training loss in epoch 152 = 0.939574122428894.   Test loss = 0.939574122428894\n",
      "Training loss in epoch 153 = 0.93464595079422.   Test loss = 0.93464595079422\n",
      "Training loss in epoch 154 = 0.9388474822044373.   Test loss = 0.9388474822044373\n",
      "Training loss in epoch 155 = 0.9336627125740051.   Test loss = 0.9336627125740051\n",
      "Training loss in epoch 156 = 0.9381281137466431.   Test loss = 0.9381281137466431\n",
      "Training loss in epoch 157 = 0.9326844811439514.   Test loss = 0.9326844811439514\n",
      "Training loss in epoch 158 = 0.9374164938926697.   Test loss = 0.9374164938926697\n",
      "Training loss in epoch 159 = 0.9317113161087036.   Test loss = 0.9317113161087036\n",
      "Training loss in epoch 160 = 0.9367124438285828.   Test loss = 0.9367124438285828\n",
      "Training loss in epoch 161 = 0.9307428002357483.   Test loss = 0.9307428002357483\n",
      "Training loss in epoch 162 = 0.9360162615776062.   Test loss = 0.9360162615776062\n",
      "Training loss in epoch 163 = 0.9297789335250854.   Test loss = 0.9297789335250854\n",
      "Training loss in epoch 164 = 0.9353280663490295.   Test loss = 0.9353280663490295\n",
      "Training loss in epoch 165 = 0.9288198351860046.   Test loss = 0.9288198351860046\n",
      "Training loss in epoch 166 = 0.9346479177474976.   Test loss = 0.9346479177474976\n",
      "Training loss in epoch 167 = 0.9278653860092163.   Test loss = 0.9278653860092163\n",
      "Training loss in epoch 168 = 0.9339760541915894.   Test loss = 0.9339760541915894\n",
      "Training loss in epoch 169 = 0.9269159436225891.   Test loss = 0.9269159436225891\n",
      "Training loss in epoch 170 = 0.933312714099884.   Test loss = 0.933312714099884\n",
      "Training loss in epoch 171 = 0.9259713888168335.   Test loss = 0.9259713888168335\n",
      "Training loss in epoch 172 = 0.9326579570770264.   Test loss = 0.9326579570770264\n",
      "Training loss in epoch 173 = 0.9250317215919495.   Test loss = 0.9250317215919495\n",
      "Training loss in epoch 174 = 0.9320120215415955.   Test loss = 0.9320120215415955\n",
      "Training loss in epoch 175 = 0.9240970611572266.   Test loss = 0.9240970611572266\n",
      "Training loss in epoch 176 = 0.9313750863075256.   Test loss = 0.9313750863075256\n",
      "Training loss in epoch 177 = 0.9231675267219543.   Test loss = 0.9231675267219543\n",
      "Training loss in epoch 178 = 0.930747389793396.   Test loss = 0.930747389793396\n",
      "Training loss in epoch 179 = 0.9222432374954224.   Test loss = 0.9222432374954224\n",
      "Training loss in epoch 180 = 0.9301288723945618.   Test loss = 0.9301288723945618\n",
      "Training loss in epoch 181 = 0.9213241934776306.   Test loss = 0.9213241934776306\n",
      "Training loss in epoch 182 = 0.9295203685760498.   Test loss = 0.9295203685760498\n",
      "Training loss in epoch 183 = 0.9204105138778687.   Test loss = 0.9204105138778687\n",
      "Training loss in epoch 184 = 0.9289218783378601.   Test loss = 0.9289218783378601\n",
      "Training loss in epoch 185 = 0.9195021390914917.   Test loss = 0.9195021390914917\n",
      "Training loss in epoch 186 = 0.9228907227516174.   Test loss = 0.9228907227516174\n",
      "Training loss in epoch 187 = 0.9178242683410645.   Test loss = 0.9178242683410645\n",
      "Training loss in epoch 188 = 0.919560968875885.   Test loss = 0.919560968875885\n",
      "Training loss in epoch 189 = 0.9170469641685486.   Test loss = 0.9170469641685486\n",
      "Training loss in epoch 190 = 0.9128963947296143.   Test loss = 0.9128963947296143\n",
      "Training loss in epoch 191 = 0.9175328612327576.   Test loss = 0.9175328612327576\n",
      "Training loss in epoch 192 = 0.9120541214942932.   Test loss = 0.9120541214942932\n",
      "Training loss in epoch 193 = 0.9165263175964355.   Test loss = 0.9165263175964355\n",
      "Training loss in epoch 194 = 0.9106716513633728.   Test loss = 0.9106716513633728\n",
      "Training loss in epoch 195 = 0.9155343174934387.   Test loss = 0.9155343174934387\n",
      "Training loss in epoch 196 = 0.9092962145805359.   Test loss = 0.9092962145805359\n",
      "Training loss in epoch 197 = 0.9145575761795044.   Test loss = 0.9145575761795044\n",
      "Training loss in epoch 198 = 0.9079281687736511.   Test loss = 0.9079281687736511\n",
      "Training loss in epoch 199 = 0.9135962128639221.   Test loss = 0.9135962128639221\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, learning_rate, n_epochs)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fee7908",
   "metadata": {},
   "source": [
    "# Weight Dubugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "252e6b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'w:0' shape=(3, 3) dtype=float32, numpy=\n",
      "array([[-0.29817116, -0.13804616, -0.6748989 ],\n",
      "       [ 0.05275182, -0.6625073 ,  0.11678936],\n",
      "       [ 0.0941266 ,  0.0452926 ,  0.0350436 ]], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "print(model._layers[0].w) # Input Layer weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c958e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033bc71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bf9412a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2988e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
