{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f6a9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Funcs\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as ImagePIL\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from numpy.random import randint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623bd458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "from scipy import stats\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98676dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import Reshape\n",
    "from keras import backend\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.constraints import Constraint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c72792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Funcs\n",
    "from Unpack_Scaffold_Data import readAndOutputDataset, curveVisualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cc1aff",
   "metadata": {},
   "source": [
    "# Data Read Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4b33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_path = \"/Users/zacharyg/Documents/GitHub/fundemental-neural-nets/GANS/Scaffold_GAN/scaffold_dataset_WU_LAB/Prints\"\n",
    "modulus_path = \"/Users/zacharyg/Documents/GitHub/fundemental-neural-nets/GANS/Scaffold_GAN/scaffold_dataset_WU_LAB/Prints/modulus_data_types.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e90761",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, y_df, file_order = readAndOutputDataset(curve_path, modulus_path, reverse=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09eb474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity Check\n",
    "print(\"X SHAPE:\", X.shape);\n",
    "print(\"y SHAPE:\", y.shape);\n",
    "print();\n",
    "\n",
    "\n",
    "# Visualization\n",
    "# curveVisualization(X, y, file_order);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8ec86",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c54a1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposeStressData(X_Data):\n",
    "    X = [];\n",
    "    \n",
    "    for data in X_Data:\n",
    "        X.append(data.T);\n",
    "        \n",
    "    return np.array(X);\n",
    "\n",
    "def normalizeStressStrain(x):\n",
    "    for curve_index in range(len(x)):\n",
    "        curve = x[curve_index];\n",
    "        \n",
    "        max_stress_val = np.max(curve[0]);\n",
    "        max_strain_val = np.max(curve[1]);\n",
    "        \n",
    "        curve[0] = curve[0] / max_stress_val;\n",
    "        curve[1] = curve[1] / max_strain_val;\n",
    "        \n",
    "    return x;\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    \n",
    "    Parameters\n",
    "    -----------------\n",
    "    x: Array of Homogenous (RGB) values of input data \n",
    "    \n",
    "    Returns\n",
    "    -----------------\n",
    "    new_imgs: (numpy integer array) Numpy array of normalized data\n",
    "    \"\"\"\n",
    "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "def stringtoCategorical(y):    \n",
    "    data = [];\n",
    "    \n",
    "    for type_index in range(len(y)):\n",
    "        wrd = y[type_index];\n",
    "        encoding = 0.0;\n",
    "        \n",
    "        if (wrd == \"Cubic\"):\n",
    "            encoding = 1.0;\n",
    "        elif (wrd == \"Gyroid\"):\n",
    "            encoding = 2.0;\n",
    "            \n",
    "        data.append([encoding]);\n",
    "        \n",
    "    return np.array(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9addb4ab",
   "metadata": {},
   "source": [
    "# Process Parameter Stripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87933ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameterStrip(y):\n",
    "    y_t = y.T;\n",
    "    \n",
    "    Index = y_t[0];\n",
    "    Modulus = y_t[1];\n",
    "    Spacing = y_t[2];\n",
    "    Infill = y_t[3];\n",
    "    Height = y_t[4];\n",
    "    Speed = y_t[5];\n",
    "    Temp = y_t[6];\n",
    "    Mass = y_t[7];\n",
    "    Porosity = y_t[8];\n",
    "    Type = y_t[9];\n",
    "    return Index, Modulus, Spacing, Infill, Height, Speed, Temp, Mass, Porosity, Type\n",
    "\n",
    "Index, Modulus, Spacing, Infill, Height, Speed, Temp, Mass, Porosity, Type = parameterStrip(y);\n",
    "\n",
    "def parameterStripInfill(y):\n",
    "    y_t = y.T;\n",
    "    \n",
    "    Modulus = y_t[0];\n",
    "    Porosity = y_t[1];\n",
    "    Energy_Absorb = y_t[2];\n",
    "    Height = y_t[3];\n",
    "    Spacing = y_t[4];\n",
    "    Speed = y_t[5];\n",
    "    Temp = y_t[6];\n",
    "    \n",
    "    return Modulus, Porosity, Energy_Absorb, Height, Spacing, Speed, Temp    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648df538",
   "metadata": {},
   "source": [
    "# Energy Absorption Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Energy_Absorption = [];\n",
    "\n",
    "for curve in X:\n",
    "    interval_x = curve[0];\n",
    "    interval_y = curve[1];\n",
    "    \n",
    "    val = integrate.simpson(interval_y, interval_x);\n",
    "    Energy_Absorption.append(val);\n",
    "    \n",
    "Energy_Absorption = np.array(Energy_Absorption);\n",
    "\n",
    "# Sanity Check\n",
    "print(Energy_Absorption.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99cdc416",
   "metadata": {},
   "source": [
    "# Data Division based on Infill Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf11d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organizeParameters(_Data):\n",
    "    \"\"\"\n",
    "    Desc\n",
    "    \"\"\"\n",
    "    Modulus = _Data[:, 1:2];\n",
    "    Porosity = _Data[:, 8:9];\n",
    "    Energy_Abs = _Data[:, 10:11];\n",
    "    Spacing = _Data[:, 2:3];\n",
    "    printing_params = _Data[:, 4:7];\n",
    "\n",
    "    cut_params = np.concatenate((\n",
    "        Modulus, \n",
    "        Porosity,\n",
    "        Energy_Abs,\n",
    "        Spacing,\n",
    "        printing_params\n",
    "    ), axis=1);\n",
    "    \n",
    "    return cut_params;\n",
    "\n",
    "\n",
    "Line_Data = [];\n",
    "Cubic_Data = [];\n",
    "Gyroid_Data = [];\n",
    "\n",
    "_y = cut_params = np.concatenate((\n",
    "    y,\n",
    "    (np.reshape(Energy_Absorption, (675,1))),\n",
    "), axis=1);\n",
    "\n",
    "for curve in _y:\n",
    "    if ('Gyroid' in curve):\n",
    "        Gyroid_Data.append(curve);\n",
    "    elif ('Cubic' in curve):\n",
    "        Cubic_Data.append(curve);\n",
    "    elif ('Line' in curve):\n",
    "        Line_Data.append(curve);\n",
    "        \n",
    "Line_Data = np.array(Line_Data);\n",
    "Cubic_Data = np.array(Cubic_Data);\n",
    "Gyroid_Data = np.array(Gyroid_Data);\n",
    "\n",
    "\n",
    "X_Line = organizeParameters(Line_Data);\n",
    "X_Cubic = organizeParameters(Cubic_Data);\n",
    "X_Gyroid = organizeParameters(Gyroid_Data);\n",
    "\n",
    "# Sanity Check\n",
    "print(X_Line.shape)\n",
    "print(X_Cubic.shape)\n",
    "print(X_Gyroid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c3ffe",
   "metadata": {},
   "source": [
    "# Infill Parameter Stripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61fe9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Modulus_Cubic, Porosity_Cubic, Energy_Absorb_Cubic, Height_Cubic, Spacing_Cubic, Speed_Cubic, Temp_Cubic = parameterStripInfill(X_Cubic);\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82a42d",
   "metadata": {},
   "source": [
    "# Plotting Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a901f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 675 Stress-Strain Curve Domain\n",
    "feature_domain_675 = list(range(675 + 1));\n",
    "feature_domain_675.pop(0) \n",
    "feature_domain_675 = np.repeat(feature_domain_675, 4, axis=0) # Changed to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bd3128",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_domain_8 = list(range(8 + 1));\n",
    "feature_domain_8.pop(0);\n",
    "feature_domain_8_rep = list(np.arange(1,9))*675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493c2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_domain_7 = list(range(7 + 1));\n",
    "feature_domain_7.pop(0);\n",
    "feature_domain_7_rep = list(np.arange(1,8))*675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf30a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_domain_5 = list(range(5 + 1));\n",
    "feature_domain_5.pop(0);\n",
    "feature_domain_5_rep = list(np.arange(1,6)) * 675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b08edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_domain_4 = list(range(4 + 1));\n",
    "feature_domain_4.pop(0);\n",
    "feature_domain_4_rep = list(np.arange(1,5)) * 675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d840ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_domain_2 = list(range(2 + 1));\n",
    "feature_domain_2.pop(0);\n",
    "feature_domain_2_rep = list(np.arange(1,3)) * 675"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf1f639",
   "metadata": {},
   "source": [
    "# Parameter Cutting [Fixiating on Cubic]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e696b54",
   "metadata": {},
   "source": [
    "Since temperature and Line Spacing has the heighest Spearman Correlation value, lets just fixiate on just these two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40012ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_Cubic)\n",
    "print()\n",
    "print()\n",
    "\n",
    "X_Cubic_Curve = X_Cubic[:, :3]\n",
    "X_Cubic_Printing_1 = X_Cubic[:, 6:]\n",
    "X_Cubic_Printing_2 = X_Cubic[:, 4:5]\n",
    "\n",
    "X_Cubic_Printing_ALL = X_Cubic[:, 3:7]\n",
    "\n",
    "X_Cubic_Data = np.concatenate((\n",
    "        X_Cubic_Curve, \n",
    "        X_Cubic_Printing_1,\n",
    "        X_Cubic_Printing_2,\n",
    "), axis=1);\n",
    "\n",
    "X_Cubic_Data_ALL = np.concatenate((\n",
    "        X_Cubic_Curve, \n",
    "        X_Cubic_Printing_ALL\n",
    "), axis=1);\n",
    "\n",
    "X_Cubic_Data_Reg = np.copy(X_Cubic_Data);\n",
    "\n",
    "# Incase we want to fixuate it on the same scale\n",
    "for curve in X_Cubic_Data:\n",
    "    curve[1] = curve[1] * 1000\n",
    "    curve[2] = curve[2] / 10\n",
    "    curve[4] = curve[4] * 1000\n",
    "    \n",
    "# Incase we want to fixuate it on the same scale\n",
    "for curve in X_Cubic_Data_ALL:\n",
    "    curve[1] = curve[1] * 1000\n",
    "    curve[2] = curve[2] / 10\n",
    "    curve[3] = curve[3] * 100\n",
    "    curve[4] = curve[4] * 1000\n",
    "    curve[5] = curve[5] * 10\n",
    "\n",
    "print(X_Cubic_Data)\n",
    "print()\n",
    "print(X_Cubic_Data_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4290ea",
   "metadata": {},
   "source": [
    "# Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3522dd",
   "metadata": {},
   "source": [
    "$$\n",
    "d_n = \\{Modulus, Porosity, Energy Absorption, Temperature, Line Height\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d6598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Lines Chart (DISTRIBUTION)\n",
    "fig_k = px.line(\n",
    "    x=feature_domain_5, \n",
    "    y=X_Cubic_Data[200],\n",
    "    title=\"Single Parameter Curve\",\n",
    "    labels={\"x\": \"Parameters\", \"y\":\"Normalized values (Divided by Max)\"}\n",
    ")\n",
    "\n",
    "fig_k.show()\n",
    "\n",
    "\n",
    "# Multiple Lines Chart (DISTRIBUTION)\n",
    "fig = go.Figure()\n",
    "\n",
    "for line in range(len(X_Cubic)):\n",
    "    data = X_Cubic_Data[line];\n",
    "    fig.add_trace(go.Scatter(x=feature_domain_5, y=data))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921ecdac",
   "metadata": {},
   "source": [
    "# Quick Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0242084",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max value:\", np.max(X_Cubic_Data));\n",
    "X_Cubic_Data_N = X_Cubic_Data / np.max(X_Cubic_Data);\n",
    "\n",
    "X_Cubic_Data_ALL_N = X_Cubic_Data_ALL / np.max(X_Cubic_Data_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1f225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Lines Chart (DISTRIBUTION)\n",
    "fig = go.Figure()\n",
    "\n",
    "# for line in range(len(X_Cubic)):\n",
    "#     data = X_Cubic_Data_N[line];\n",
    "#     fig.add_trace(go.Scatter(x=feature_domain_5, y=data))\n",
    "    \n",
    "for line in range(len(X_Cubic_Data_ALL_N)):\n",
    "    data = X_Cubic_Data_ALL_N[line];\n",
    "    fig.add_trace(go.Scatter(x=feature_domain_7, y=data))\n",
    "\n",
    "fig.show(renderer = \"browser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac698934",
   "metadata": {},
   "source": [
    "# Vanilla GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e673b82e",
   "metadata": {},
   "source": [
    "### Discriminator Data Sampling Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daabcadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_real_samples(dataset, n_samples):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------\n",
    "    real_dataset: dataset with the real data\n",
    "    n_samples: amount of real images to sample from\n",
    "    \n",
    "    Returns\n",
    "    --------------\n",
    "    X: samples of n images in a list\n",
    "    Y: labels of (1's) for true images (Binary Classification)\n",
    "    \"\"\"\n",
    "    if (isinstance(dataset, list)):\n",
    "        dataset = np.asarray(dataset);\n",
    "        \n",
    "    random_num = randint(0, dataset.shape[0], n_samples);\n",
    "    X = dataset[random_num];\n",
    "    y = np.ones((n_samples, 1));\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5663c64e",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2410d6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_Discriminator(in_shape=5):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential();\n",
    "    \n",
    "    model.add(Dense(10, input_dim=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(5, input_dim=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    opt = Adam(learning_rate =0.001)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer = opt, \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2239d3d1",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a505bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GAN_Generator(in_shape=5):\n",
    "    model = tf.keras.Sequential();\n",
    "    \n",
    "    model.add(Dense(5, input_dim=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(5, input_dim=in_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(5, activation=\"tanh\")) \n",
    "    \n",
    "    return model;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f98acc",
   "metadata": {},
   "source": [
    "### Summary of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be5ac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "GANdiscriminator = GAN_Discriminator();\n",
    "GANgenerator = GAN_Generator();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d337a2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "GANdiscriminator.summary();\n",
    "GANgenerator.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4326485",
   "metadata": {},
   "source": [
    "### Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latentDimensionalGenerator(latent_dimensions, n_samples, randomGaussian = False):\n",
    "    data = [];\n",
    "    \n",
    "    for sample in range(n_samples):\n",
    "        x_input_0 = np.random.randn(latent_dimensions); # Points sampled from a normalized distribution.\n",
    "        data.append(x_input_0);\n",
    "        \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70bfefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator production\n",
    "def generate_samples(g_model, latent_dim, n_samples):\n",
    "    x_input = latentDimensionalGenerator(latent_dim, n_samples)  # generate points in a latent space\n",
    "    X = g_model.predict(x_input)\n",
    "    y = np.zeros((n_samples, 1))  # create 'fake' class labels (0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f758d48",
   "metadata": {},
   "source": [
    "### GAN: Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0500660f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(generator, discriminator):\n",
    "    discriminator.trainable = False # We set the discriminator as not trainable so the generator updates\n",
    "    model = tf.keras.Sequential() \n",
    "    \n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    opt = Adam(learning_rate = 0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt) # Generator will train on this loss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8156f07d",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf706df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples, save_path=\"\"):\n",
    "    # Real Images based on discriminator\n",
    "    X_real, y_real = sample_real_samples(dataset, n_samples)\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    \n",
    "    # Fake Images based on discriminator\n",
    "    x_fake, y_fake = generate_samples(g_model, latent_dim, n_samples)\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    \n",
    "    print(\"============== CURVE GENERATION ON EPOCH\", epoch,\"==============\");\n",
    "    \n",
    "    for curve in x_fake:\n",
    "        plt.plot(feature_domain_5, curve)\n",
    "    \n",
    "    if (save_path != \"\"):\n",
    "        plt.title(\"Training in epoch: \" + str(epoch))\n",
    "        plt.savefig(os.path.join(save_path, str(epoch) + '.png'));\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03a133c9",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb9be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train_gan(g_model, d_model, gan_model, training_data, latent_dim, n_epochs, n_batch, save_path=\"\"):\n",
    "    d1Loss = [];\n",
    "    d2Loss = [];\n",
    "    gLoss = [];\n",
    "    \n",
    "    half_batch = int(n_batch / 2);\n",
    "    \n",
    "    for i in range(n_epochs):                \n",
    "        # Real Image Discriminator Training\n",
    "        X_real, y_real = sample_real_samples(training_data, half_batch)\n",
    "        d_loss1, _ = d_model.train_on_batch(X_real, y_real) # Training on real\n",
    "\n",
    "        # Fake Image Discriminator Training\n",
    "        X_fake, y_fake = generate_samples(g_model, latent_dim, half_batch)\n",
    "        d_loss2, _ = d_model.train_on_batch(X_fake, y_fake) # Training on fakes\n",
    "\n",
    "        # Create a latent space and inverted labels\n",
    "        X_gan = latentDimensionalGenerator(latent_dim, n_batch)\n",
    "        y_gan = np.ones((n_batch, 1)) # Pretend that that they are all real.\n",
    "\n",
    "        # Update the generator via the discriminator's error\n",
    "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "\n",
    "        # summarize loss on this batch\n",
    "        print('>%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "        summarize_performance(i, g_model, d_model, training_data, latent_dim, 100, save_path)\n",
    "        \n",
    "        d1Loss.append(d_loss1);\n",
    "        d2Loss.append(d_loss2);\n",
    "        gLoss.append(g_loss);\n",
    "        \n",
    "    return d1Loss, d2Loss, gLoss;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3152ad9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 5;\n",
    "gan_model = define_gan(GANgenerator, GANdiscriminator);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1661c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.getcwd());\n",
    "# os.chdir(\"/Users/zacharyg/Documents/GitHub/fundemental-neural-nets/GANS/Scaffold_GAN\");\n",
    "# image_save_path = \"./images/\"\n",
    "\n",
    "# if not os.path.exists(image_save_path):\n",
    "#     os.makedirs(image_save_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c2706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3000;\n",
    "X = X_Cubic_Data_N.astype('float32')\n",
    "\n",
    "#Training\n",
    "d1, d2, gloss = train_gan(\n",
    "    GANgenerator, \n",
    "    GANdiscriminator, \n",
    "    gan_model, \n",
    "    X, \n",
    "    latent_dim, \n",
    "    n_epochs, # n_epochs\n",
    "    5,  # batch size\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b40255",
   "metadata": {},
   "source": [
    "# Vanilla GAN Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bfc0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotCurve(X, y, title=\"Curve\", xlabel=\"Steps\", ylabel=\"Value\"):\n",
    "    x_axis = X\n",
    "    y_axis = y\n",
    "\n",
    "    plt.plot(x_axis, y_axis)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3858e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(n_epochs + 1));\n",
    "popping = epochs.pop(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d8fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCurve(epochs, d1, title=\"d1 loss\");\n",
    "plotCurve(epochs, d2, title=\"d2 loss\");\n",
    "plotCurve(epochs, gloss, title=\"GAN Loss\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2cf4ae",
   "metadata": {},
   "source": [
    "# Prediction Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838838cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_X, fake_y = generate_samples(GANgenerator, 5, 675);\n",
    "\n",
    "transformed_fake_X = fake_X * np.max(X_Cubic_Data)\n",
    "\n",
    "for curve in transformed_fake_X:\n",
    "    curve[1] = curve[1] / 1000\n",
    "    curve[2] = curve[2] * 10\n",
    "    curve[4] = curve[4] / 1000\n",
    "\n",
    "print(transformed_fake_X)\n",
    "print()\n",
    "\n",
    "print(\"Actual:\", X_Cubic[20]);\n",
    "\n",
    "# All REAL Lines (DISTRIBUTION)\n",
    "fig = go.Figure()\n",
    "\n",
    "for line in range(len(X_Cubic)):\n",
    "    data = X_Cubic_Data_Reg[line];\n",
    "    fig.add_trace(go.Scatter(x=feature_domain_5, y=data))\n",
    "\n",
    "fig.show()\n",
    "    \n",
    "# All GENERATED Lines (DISTRIBUTION)\n",
    "fig = go.Figure()\n",
    "\n",
    "for line in range(len(X_Cubic)):\n",
    "    data = transformed_fake_X[line];\n",
    "    fig.add_trace(go.Scatter(x=feature_domain_5, y=data))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba78c2",
   "metadata": {},
   "source": [
    "# K-Means CGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58d5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2e87c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max value:\", np.max(X_Cubic_Data));\n",
    "X_Cubic_Data_N = X_Cubic_Data / np.max(X_Cubic_Data);\n",
    "\n",
    "print(X_Cubic_Data_N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e244d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stripParams_5(y):\n",
    "    M = [];\n",
    "    P = [];\n",
    "    E = [];\n",
    "    T = [];\n",
    "    H = [];\n",
    "    \n",
    "    for curve in y:\n",
    "        M.append(curve[0]);\n",
    "        P.append(curve[1]);\n",
    "        E.append(curve[2]);\n",
    "        T.append(curve[3]);\n",
    "        H.append(curve[4]);\n",
    "        \n",
    "    return np.array(M), np.array(P), np.array(E), np.array(T), np.array(H)\n",
    "\n",
    "# Mod_N, Por_N, Engy_N, Temp_N, Height_N = stripParams_5(X_Cubic_Data_N);\n",
    "\n",
    "def stripParams_7(y):\n",
    "    M = [];\n",
    "    P = [];\n",
    "    E = [];\n",
    "    Spacing = [];\n",
    "    H = [];\n",
    "    Speed = [];\n",
    "    T = [];\n",
    "    \n",
    "    \n",
    "    for curve in y:\n",
    "        M.append(curve[0]);\n",
    "        P.append(curve[1]);\n",
    "        E.append(curve[2]);\n",
    "        Spacing.append(curve[3]);\n",
    "        H.append(curve[4]);\n",
    "        Speed.append(curve[5]);\n",
    "        T.append(curve[6]);\n",
    "        \n",
    "        \n",
    "    return np.array(M), np.array(P), np.array(E), np.array(Spacing), np.array(H), np.array(Speed), np.array(T), \n",
    "\n",
    "Mod_N, Por_N, Engy_N, Spacing_N, Height_N, Speed_N, Temp_N,  = stripParams_7(X_Cubic_Data_ALL_N);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d10b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve_3D_Cluster = X_Cubic_Data_N[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ae46f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Curve_3D_Cluster = X_Cubic_Data_ALL_N[:, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b50f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kde = KernelDensity(kernel='gaussian', bandwidth = 3).fit(Engy_N.reshape(-1, 1))\n",
    "kmeans = KMeans(n_clusters=3)                   # Number of clusters == 3\n",
    "kmeans = kmeans.fit(Curve_3D_Cluster)                          # Fitting the input data\n",
    "labels = kmeans.predict(Curve_3D_Cluster)                      # Getting the cluster labels\n",
    "centroids = kmeans.cluster_centers_             # Centroid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0797ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "x = np.array(labels==0)\n",
    "y = np.array(labels==1)\n",
    "z = np.array(labels==2)\n",
    "\n",
    "\n",
    "ax.scatter(Curve_3D_Cluster[x][:, 0], Curve_3D_Cluster[x][:, 1], Curve_3D_Cluster[x][:, 2], color='red')\n",
    "ax.scatter(Curve_3D_Cluster[y][:, 0], Curve_3D_Cluster[y][:, 1], Curve_3D_Cluster[y][:, 2], color='blue')\n",
    "ax.scatter(Curve_3D_Cluster[z][:, 0], Curve_3D_Cluster[z][:, 1], Curve_3D_Cluster[z][:, 2], color='yellow')\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2],\n",
    "            marker='x', s=169, linewidths=10,\n",
    "            color='black', zorder=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb277144",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Curve_3D_Cluster[x][:, 0].shape)\n",
    "print(Curve_3D_Cluster[y][:, 0].shape)\n",
    "print(Curve_3D_Cluster[z][:, 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb25101",
   "metadata": {},
   "outputs": [],
   "source": [
    "RED = np.empty(41)\n",
    "RED.fill(0)\n",
    "\n",
    "BLUE = np.empty(73)\n",
    "BLUE.fill(1)\n",
    "\n",
    "YELLOW = np.empty(111)\n",
    "YELLOW.fill(2)\n",
    "\n",
    "Name_Arr = np.concatenate((RED, BLUE, YELLOW), axis=0)\n",
    "print(Name_Arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b274a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.concatenate((Curve_3D_Cluster[x][:, 0], Curve_3D_Cluster[y][:, 0], Curve_3D_Cluster[z][:, 0]), axis=0)\n",
    "B = np.concatenate((Curve_3D_Cluster[x][:, 1], Curve_3D_Cluster[y][:, 1], Curve_3D_Cluster[z][:, 1]), axis=0)\n",
    "C = np.concatenate((Curve_3D_Cluster[x][:, 2], Curve_3D_Cluster[y][:, 2], Curve_3D_Cluster[z][:, 2]), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5ae428",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(x=A, y=B, z=C, color=Name_Arr)\n",
    "fig.show(renderer=\"browser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de860a00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Curve_3D_Cluster = np.array(Curve_3D_Cluster)\n",
    "print(\"All index value is: \", np.where(Curve_3D_Cluster == Curve_3D_Cluster[z][6][0]))\n",
    "print()\n",
    "print(Curve_3D_Cluster[z][6])\n",
    "print(X_Cubic_Data_N[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9753c4a",
   "metadata": {},
   "source": [
    "### Label Data based on K-Means Centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6387f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceBasedOnKMeans(org, clustered, label):\n",
    "    data = [];\n",
    "    \n",
    "    for data_index in range(len(clustered)):\n",
    "        curve = clustered[data_index][0];\n",
    "        ind, discard = np.where(org == curve);\n",
    "    \n",
    "        temp = np.copy(org[ind][0]);\n",
    "#         temp = np.append(temp, label)\n",
    "        data.append(temp);\n",
    "    return np.array(data);\n",
    "\n",
    "# Cluster_1 = replaceBasedOnKMeans(X_Cubic_Data_N, Curve_3D_Cluster[x], \"A\");\n",
    "# Cluster_2 = replaceBasedOnKMeans(X_Cubic_Data_N, Curve_3D_Cluster[y], \"B\");\n",
    "# Cluster_3 = replaceBasedOnKMeans(X_Cubic_Data_N, Curve_3D_Cluster[z], \"C\");\n",
    "\n",
    "Cluster_1 = replaceBasedOnKMeans(X_Cubic_Data_ALL_N, Curve_3D_Cluster[x], \"A\");\n",
    "Cluster_2 = replaceBasedOnKMeans(X_Cubic_Data_ALL_N, Curve_3D_Cluster[y], \"B\");\n",
    "Cluster_3 = replaceBasedOnKMeans(X_Cubic_Data_ALL_N, Curve_3D_Cluster[z], \"C\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f17c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune the data\n",
    "Cluster_A = Cluster_1[:, 3:];\n",
    "Cluster_B = Cluster_2[:, 3:];\n",
    "Cluster_C = Cluster_3[:, 3:];\n",
    "\n",
    "cluster_X = np.concatenate((Cluster_A, Cluster_B, Cluster_C), axis=0);\n",
    "print(cluster_X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae5aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneCategorical(y):\n",
    "    arr = [];\n",
    "    for data in y:\n",
    "        if (data == 0.0):\n",
    "            arr.append([0.0]);\n",
    "        elif (data == 1.0):\n",
    "            arr.append([1.0]);\n",
    "        elif (data == 2.0):\n",
    "            arr.append([2.0]);\n",
    "    \n",
    "    return np.array(arr);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe80383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hot_encoded_curves = tf.keras.utils.to_categorical(Name_Arr, num_classes = 3);\n",
    "hot_encoded_curves = oneCategorical(Name_Arr);\n",
    "print(hot_encoded_curves)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a52f487",
   "metadata": {},
   "source": [
    "### Pairwise Data Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21108fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [];\n",
    "\n",
    "for param_index in range(len(cluster_X)):\n",
    "    data = cluster_X[param_index]\n",
    "    category = hot_encoded_curves[param_index]\n",
    "    data = data.astype('float32')\n",
    "    category = category.astype('float32')\n",
    "    payload = [data, category]\n",
    "    X.append(payload);\n",
    "    \n",
    "# print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f06f22a",
   "metadata": {},
   "source": [
    "### Tensorflow settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52339dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0edcc4",
   "metadata": {},
   "source": [
    "### Discriminator Data Sampling Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414f8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_real_samples(dataset, n_samples):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------\n",
    "    dataset: dataset with the real data\n",
    "    cond_data: the data that is conditioned with the GAN\n",
    "    n_samples: amount of real images to sample from\n",
    "    \n",
    "    Returns\n",
    "    --------------\n",
    "    X: samples of n images in a list\n",
    "    Y: labels of (1's) for true images (Binary Classification)\n",
    "    \"\"\"\n",
    "    params = [];\n",
    "    labels = [];\n",
    "    \n",
    "    for sample in range(n_samples):\n",
    "        randVal = random.choice(dataset)\n",
    "        params.append(randVal[0].astype('float32'));\n",
    "        labels.append(randVal[1].astype('float32'));\n",
    "    y = np.ones((n_samples, 1));\n",
    "    \n",
    "    return [params, labels], y\n",
    "\n",
    "[P, O], B = sample_real_samples(X, 10)\n",
    "print(O)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271f703",
   "metadata": {},
   "source": [
    "### Custom Loss Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f6b247",
   "metadata": {},
   "source": [
    "BCE_Regularized:\n",
    "$$\n",
    "L_{BCE} = -\\dfrac{1}{n} \\sum_{i=1}^{n} y_{i} \\cdot \\log \\hat{y}_i + (1 - y_i) \\cdot \\log (1-\\hat{y}_i) + \\left[ \\lambda \\cdot \\sum_{i=1}^{n} W_i^2 \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ed82e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8707dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def BCE_Regularized(y_true, y_pred, l2_factor):\n",
    "#     bce = tf.keras.losses.BinaryFocalCrossentropy();\n",
    "#     constraint = \n",
    "#     return bce(y_true, y_pred) + "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc37a4c",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464a21be",
   "metadata": {},
   "source": [
    "Remember that the Objective Function this time is:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\min_{G}\\max_{D}V(D,G) = \\mathbb{E}_{x \\text{-} p_{data}(x)}[\\log D(x | y)]\n",
    "+ \\mathbb{E}_{z \\text{-} p_{z}(z)} [\\log (1 - D(G(z | y))]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Such that $y$ is a auxillary data. In this case, its the infill type (One hot encoded) which helps better learn the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0f3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalDiscriminator(in_shape=4, num_classes=3):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    in_label = tf.keras.Input(shape=(1,))\n",
    "    embed = tf.keras.layers.Embedding(num_classes, 10)(in_label) # Keep the embedding layers low...\n",
    "    cond_y = tf.keras.layers.Dense(4)(embed)\n",
    "    cond_y = tf.keras.layers.Reshape((4,))(cond_y)\n",
    "    \n",
    "    in_parameters = tf.keras.Input(shape=in_shape)\n",
    "    merge = tf.keras.layers.Concatenate()([in_parameters, cond_y])\n",
    "    x = tf.keras.layers.Dense(100, input_dim=in_shape)(merge)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tf.keras.layers.Dense(50)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tf.keras.layers.Dense(4)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tf.keras.layers.Dense(4)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    out = tf.keras.layers.Dense(1, activation='sigmoid')(x) # Output layer\n",
    "    \n",
    "    model = tf.keras.Model([in_parameters, in_label], out)\n",
    "    \n",
    "    opt = Adam(learning_rate = 0.0002)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer = opt, \n",
    "        metrics=['accuracy']\n",
    "    );\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f436c3",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621af045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditionalGenerator(in_shape=4, num_classes=3):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    in_label = tf.keras.Input(shape=(1,))\n",
    "    embed = tf.keras.layers.Embedding(num_classes, 10)(in_label) # Keep the embedding layers low...\n",
    "    cond_y = tf.keras.layers.Dense(4)(embed)\n",
    "    cond_y = tf.keras.layers.Reshape((4,))(cond_y)\n",
    "    \n",
    "    in_noise = tf.keras.Input(shape=in_shape)\n",
    "    merge = tf.keras.layers.Concatenate()([in_noise, cond_y])\n",
    "    x = tf.keras.layers.Dense(100)(merge)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tf.keras.layers.Dense(4)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tf.keras.layers.Dense(4)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    out = tf.keras.layers.Dense(4, input_dim=in_shape, activation='tanh')(x)\n",
    "    \n",
    "    model = tf.keras.Model([in_noise, in_label], out)\n",
    "    \n",
    "    return model;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ee477",
   "metadata": {},
   "source": [
    "### Summary of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a237efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = conditionalDiscriminator();\n",
    "generator = conditionalGenerator(4);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee03a0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary();\n",
    "generator.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f84b695",
   "metadata": {},
   "source": [
    "### Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5ad9d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latentDimensionalGenerator(latent_dimensions, n_samples, randomGaussian = False):\n",
    "    data = [];\n",
    "    y_cond_data = [];\n",
    "    \n",
    "    for sample in range(n_samples):\n",
    "        x_input_0 = np.random.randn(latent_dimensions); # Points sampled from a normalized distribution.\n",
    "        data.append(x_input_0);\n",
    "        y_cond_data.append([float(random.randint(0, 2))])\n",
    "        \n",
    "    return np.array(data), np.array(y_cond_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0488157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator production\n",
    "def generate_samples(g_model, latent_dim, n_samples):\n",
    "    x_input, y_cond = latentDimensionalGenerator(latent_dim, n_samples)  # generate points in a latent space\n",
    "    X = g_model.predict([x_input, y_cond])\n",
    "    y = np.zeros((n_samples, 1))  # create 'fake' class labels (0)\n",
    "    return [X, y_cond], y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d613a",
   "metadata": {},
   "source": [
    "### Visualizing the latent dimensional space in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e039673",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_X, fake_y = generate_samples(generator, 4, 10);\n",
    "print(len(fake_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for curve in fake_X[0]:\n",
    "#     fig = go.Figure();\n",
    "#     fig.add_trace(go.Scatter(x=feature_domain_2, y=curve));\n",
    "#     fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b7ee0c",
   "metadata": {},
   "source": [
    "### GAN: Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37231eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(generator, discriminator):\n",
    "    discriminator.trainable = False # We set the discriminator as not trainable so the generator updates\n",
    "\n",
    "    z, y_label = generator.input\n",
    "    \n",
    "    gen_output = generator.output\n",
    "    \n",
    "    gan_output = discriminator([gen_output, y_label])\n",
    "    \n",
    "    # Catch good or bad outputs before feeding it into the model\n",
    "    \n",
    "    \n",
    "    model = tf.keras.Model([z, y_label], gan_output)\n",
    "    \n",
    "    opt = Adam(learning_rate = 0.0002)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ecc3af2",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50548296",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples, save_path=\"\"):\n",
    "    # Real Images based on discriminator\n",
    "    [X_real, real_labels], y_real = sample_real_samples(dataset, n_samples)\n",
    "    _, acc_real = d_model.evaluate([tf.stack(X_real), tf.stack(real_labels)], y_real, verbose=0)\n",
    "    \n",
    "    # Fake Images based on discriminator\n",
    "    [X_fake, labels], y_fake = generate_samples(g_model, latent_dim, n_samples)\n",
    "    _, acc_fake = d_model.evaluate([tf.stack(X_fake), tf.stack(labels)], y_fake, verbose=0)\n",
    "    \n",
    "    print(\"============== CURVE GENERATION ON EPOCH\", epoch,\"==============\");\n",
    "    \n",
    "    for curve in X_fake:\n",
    "        plt.plot(feature_domain_4, curve)\n",
    "    \n",
    "    if (save_path != \"\"):\n",
    "        plt.title(\"Training in epoch: \" + str(epoch))\n",
    "        plt.savefig(os.path.join(save_path, str(epoch) + '.png'));\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e93163e",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e8c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train_gan(g_model, d_model, gan_model, training_data, latent_dim, n_epochs, n_batch, save_path=\"\"):\n",
    "    d1Loss = [];\n",
    "    d2Loss = [];\n",
    "    gLoss = [];\n",
    "    \n",
    "    half_batch = int(n_batch / 2);\n",
    "    \n",
    "    for i in range(n_epochs):                \n",
    "        # Real Image Discriminator Training\n",
    "        [X_real, real_labels], y_real = sample_real_samples(training_data, half_batch) # Note X_Real is [data, labels]\n",
    "        d_loss1, _ = d_model.train_on_batch([tf.stack(X_real), tf.stack(real_labels)], y_real) # Training on real\n",
    "\n",
    "        # Fake Image Discriminator Training\n",
    "        [X_fake, labels], y_fake = generate_samples(g_model, latent_dim, half_batch)\n",
    "        d_loss2, _ = d_model.train_on_batch([tf.stack(X_fake), tf.stack(labels)], y_fake) # Training on fakes\n",
    "\n",
    "        # Create a latent space and inverted labels\n",
    "        noise_z, labels = latentDimensionalGenerator(latent_dim, n_batch) # Latent space generation\n",
    "        y_gan = np.ones((n_batch, 1)) # Pretend that that they are all real.\n",
    "\n",
    "        # Update the generator via the discriminator's error\n",
    "        g_loss = gan_model.train_on_batch([tf.stack(noise_z), tf.stack(labels)], y_gan)\n",
    "\n",
    "        # summarize loss on this batch\n",
    "        print('>%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "        summarize_performance(i, g_model, d_model, training_data, latent_dim, 100, save_path)\n",
    "        \n",
    "        d1Loss.append(d_loss1);\n",
    "        d2Loss.append(d_loss2);\n",
    "        gLoss.append(g_loss);\n",
    "        \n",
    "    return d1Loss, d2Loss, gLoss;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c053e",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 4;\n",
    "gan_model = define_gan(generator, discriminator);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c15795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000;\n",
    "# X = cut_params_N.astype('float32')\n",
    "\n",
    "#Training\n",
    "d1, d2, gloss = train_gan(\n",
    "    generator, \n",
    "    discriminator, \n",
    "    gan_model, \n",
    "    X, \n",
    "    latent_dim, \n",
    "    n_epochs, # n_epochs\n",
    "    3,  # batch size\n",
    "#     save_path = image_save_path\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07cf7d88",
   "metadata": {},
   "source": [
    "# Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8720ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a028ef15",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46448cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "[X_fake, labels], y_fake = generate_samples(generator, 4, 100)\n",
    "\n",
    "for data in X_fake:\n",
    "    data = data * np.max(X_Cubic_Data);\n",
    "    print(\"[\", (data[1] / 100), (data[1] / 1000), (data[2] / 10), (data[3]), \"]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4052a7",
   "metadata": {},
   "source": [
    "# [Tensorflow-Hybrid] Conditional WGAN-GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92303c5",
   "metadata": {},
   "source": [
    "### Modified Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944b627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_real_samples(dataset, n_samples):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------\n",
    "    dataset: dataset with the real data\n",
    "    cond_data: the data that is conditioned with the GAN\n",
    "    n_samples: amount of real images to sample from\n",
    "    \n",
    "    Returns\n",
    "    --------------\n",
    "    X: samples of n images in a list\n",
    "    Y: labels of (1's) for true images (Binary Classification)\n",
    "    \"\"\"\n",
    "    params = [];\n",
    "    labels = [];\n",
    "    \n",
    "    for sample in range(n_samples):\n",
    "        randVal = random.choice(dataset)\n",
    "        params.append(randVal[0].astype('float32'));\n",
    "        labels.append(randVal[1].astype('float32'));\n",
    "    y = np.ones((n_samples, 1));\n",
    "    \n",
    "    return [params, labels], y\n",
    "\n",
    "def latentDimensionalGenerator(latent_dimensions, n_samples, randomGaussian = False):\n",
    "    data = [];\n",
    "    y_cond_data = [];\n",
    "    \n",
    "    for sample in range(n_samples):\n",
    "        x_input_0 = np.random.randn(latent_dimensions); # Points sampled from a normalized distribution.\n",
    "        data.append(x_input_0);\n",
    "        y_cond_data.append([float(random.randint(0, 2))])\n",
    "        \n",
    "    return np.array(data), np.array(y_cond_data)\n",
    "\n",
    "# Generator production\n",
    "def generate_samples(g_model, latent_dim, n_samples):\n",
    "    x_input, y_cond = latentDimensionalGenerator(latent_dim, n_samples)  # generate points in a latent space\n",
    "    X = g_model.predict([x_input, y_cond])\n",
    "    y = -np.ones((n_samples, 1))  # create 'fake' class labels (0)\n",
    "    return [X, y_cond], y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee07ef8",
   "metadata": {},
   "source": [
    "### Lipshitz continuity enforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2364adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weight Clipping (Said it was terrible to use, by the authors)\n",
    "class ClipConstraint(Constraint):\n",
    "    # set clip value when initialized\n",
    "    def __init__(self, clip_value):\n",
    "        self.clip_value = clip_value\n",
    "\n",
    "    # clip model weights to hypercube\n",
    "    def __call__(self, weights):\n",
    "        return backend.clip(weights, -self.clip_value, self.clip_value)\n",
    "\n",
    "    # get the config\n",
    "    def get_config(self):\n",
    "        return {'clip_value': self.clip_value}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d451b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP - Gradient Penalty method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1d05fb",
   "metadata": {},
   "source": [
    "### Custom loss function for Wasserstein Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f54b5b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff763b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0264b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_wasserstein_loss(y_true, y_pred):\n",
    "    real_loss = tf.reduce_mean(y_true)\n",
    "    fake_loss = tf.reduce_mean(y_pred)\n",
    "    return fake_loss - real_loss;\n",
    "\n",
    "def d_wasserstein_loss_2(y_true, y_pred):\n",
    "    return K.mean(y_true * y_pred)\n",
    "\n",
    "def g_wasserstein_loss(fake_data):\n",
    "    return -tf.reduce_mean(fake_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad08486f",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3e372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.RMSprop(learning_rate = 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361bcdc3",
   "metadata": {},
   "source": [
    "### Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a504fca",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c95581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_Wasserstein_Discriminator(in_shape=2, num_classes=3):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Some initalizers\n",
    "    w_init = tf.keras.initializers.GlorotUniform(seed=10);\n",
    "    clip = ClipConstraint(0.01);\n",
    "    tf.random.set_seed(5);\n",
    "    \n",
    "    in_label = tf.keras.Input(shape=(1,))\n",
    "    embed = tf.keras.layers.Embedding(num_classes, 10)(in_label) # Keep the embedding layers low...\n",
    "    cond_y = tf.keras.layers.Dense(2)(embed)\n",
    "    cond_y = tf.keras.layers.Reshape((2,))(cond_y)\n",
    "    \n",
    "    in_parameters = tf.keras.Input(shape=in_shape)\n",
    "    merge = tf.keras.layers.Concatenate()([in_parameters, cond_y])\n",
    "    x = tf.keras.layers.Dense(100, kernel_initializer = w_init, kernel_constraint=clip, input_dim=in_shape)(merge)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tf.keras.layers.Dense(50, kernel_initializer = w_init, kernel_constraint=clip,)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tf.keras.layers.Dense(2, kernel_initializer = w_init, kernel_constraint=clip,)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tf.keras.layers.Dense(2, kernel_initializer= w_init, kernel_constraint=clip,)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "    out = tf.keras.layers.Dense(1, kernel_initializer= w_init, kernel_constraint=clip,)(x) # Output layer\n",
    "    \n",
    "    model = tf.keras.Model([in_parameters, in_label], out)\n",
    "    \n",
    "    opt = tf.keras.optimizers.RMSprop(learning_rate = 0.0001)\n",
    "    model.compile(\n",
    "        loss = d_wasserstein_loss_2, \n",
    "        optimizer = opt\n",
    "    );\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff9924",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e3b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_Wasserstein_Generator(in_shape=2, num_classes=3):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    w_init = tf.keras.initializers.GlorotUniform(seed=10)\n",
    "    clip = ClipConstraint(0.01);\n",
    "\n",
    "    \n",
    "    in_label = tf.keras.Input(shape=(1,))\n",
    "    embed = tf.keras.layers.Embedding(num_classes, 10)(in_label) # Keep the embedding layers low...\n",
    "    cond_y = tf.keras.layers.Dense(2)(embed)\n",
    "    cond_y = tf.keras.layers.Reshape((2,))(cond_y)\n",
    "    \n",
    "    in_noise = tf.keras.Input(shape=in_shape)\n",
    "    merge = tf.keras.layers.Concatenate()([in_noise, cond_y])\n",
    "    x = tf.keras.layers.Dense(100, kernel_initializer = w_init, kernel_constraint=clip)(merge)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    x = tf.keras.layers.Dense(2, kernel_initializer = w_init, kernel_constraint=clip)(x)\n",
    "    x = tf.keras.layers.LeakyReLU(alpha=0.3)(x)\n",
    "    out = tf.keras.layers.Dense(2, kernel_initializer = w_init, kernel_constraint=clip, activation='tanh')(x)\n",
    "    \n",
    "    model = tf.keras.Model([in_noise, in_label], out)\n",
    "    \n",
    "    return model;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf6ff91",
   "metadata": {},
   "source": [
    "### WGAN-GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748afc74",
   "metadata": {},
   "source": [
    "`train_on_batch` - Updates loss on that given batch size instead of something fixed (which is nice!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d2b2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN_GP(tf.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim=2,\n",
    "        d_extra_steps=5,\n",
    "        gp_weight=10.0,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs);\n",
    "        \n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_extra_steps = d_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "    \n",
    "    def compile(self, g_optimizer, g_loss_fn):\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "        \n",
    "    def produceSampleGraphs(self):\n",
    "        # Produce 255 Sample graphs given the data\n",
    "        noise_z, labels = latentDimensionalGenerator(self.latent_dim, 255) # Latent space generation\n",
    "        generated_parameters = self.generator([noise_z, labels], training=True)\n",
    "        tensor_generated = generated_parameters.numpy();\n",
    "        for curve in tensor_generated:\n",
    "            plt.plot(feature_domain_2, curve);\n",
    "        plt.show(); \n",
    "        return;\n",
    "    \n",
    "    #### GENERATOR\n",
    "    @tf.function\n",
    "    def _generator_train(self, noise_z, labels, pretend_real):\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_parameters = self.generator([noise_z, labels], training=True)\n",
    "            # Get the discriminator logits for fake data\n",
    "            self.discriminator.trainable = False; # we don't train the discriminator\n",
    "            gen_logits = self.discriminator(\n",
    "                [tf.stack(generated_parameters), tf.stack(labels)],\n",
    "                pretend_real\n",
    "            );\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_logits)\n",
    "            \n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradients = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(zip(gen_gradients, self.generator.trainable_variables))\n",
    "        \n",
    "        return g_loss;\n",
    "    \n",
    "    def train_step(self, real_data, batch_size):\n",
    "        d_loss1 = \"NAN\";\n",
    "        d_loss2 = \"NAN\";\n",
    "        g_loss = \"NAN\";\n",
    "        \n",
    "        print(\"Training Discriminator | Epoch:\")\n",
    "        \n",
    "        # Train the discriminator for nth extra steps\n",
    "        for i in range(self.d_extra_steps):\n",
    "            # Real Image Discriminator Training\n",
    "            [X_real, real_labels], y_real = sample_real_samples(real_data, batch_size) # Note X_Real is [data, labels]\n",
    "            d_loss1 = self.discriminator.train_on_batch([tf.stack(X_real), tf.stack(real_labels)], y_real) # Training on real\n",
    "\n",
    "            # Fake Image Discriminator Training\n",
    "            [X_fake, fake_labels], y_fake = generate_samples(self.generator, self.latent_dim, batch_size)\n",
    "            d_loss2 = self.discriminator.train_on_batch([tf.stack(X_fake), tf.stack(fake_labels)], y_fake) # Training on fakes\n",
    "            \n",
    "        # Create a latent space and inverted labels\n",
    "        noise_z, labels = latentDimensionalGenerator(self.latent_dim, batch_size) # Latent space generation\n",
    "        pretend_real = np.ones((batch_size, 1))\n",
    "        \n",
    "        print()\n",
    "        print(\"Training Generator | Epoch:\")\n",
    "        \n",
    "        g_loss = self._generator_train(noise_z, labels, pretend_real)\n",
    "        \n",
    "        return {\"d_loss1\": d_loss1, \"d_loss2\": d_loss2, \"g_loss\": g_loss.numpy()}\n",
    "        \n",
    "    def fit(self, n_epochs, batch_size, real_data):\n",
    "        for _epochs in range(n_epochs):\n",
    "            losses = self.train_step(real_data, batch_size);\n",
    "            print(losses)\n",
    "            print()\n",
    "            self.produceSampleGraphs();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c45e16",
   "metadata": {},
   "source": [
    "### Training Loop 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457713e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_wgan(generator, discriminator):\n",
    "    discriminator.trainable = False # We set the discriminator as not trainable so the generator updates\n",
    "\n",
    "    z, y_label = generator.input\n",
    "    \n",
    "    gen_output = generator.output\n",
    "    gan_output = discriminator([gen_output, y_label])\n",
    "    \n",
    "    model = tf.keras.Model([z, y_label], gan_output)\n",
    "    \n",
    "    opt = Adam(learning_rate = 0.0001)\n",
    "    model.compile(loss=g_wasserstein_loss, optimizer=generator_optimizer)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d08c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "  def train_step(g_model, d_model, gan_model, real_data, batch_size, d_extra_steps, latent_dim, _epochs):\n",
    "        d_loss1 = \"NAN\";\n",
    "        d_loss2 = \"NAN\";\n",
    "        g_loss = \"NAN\";\n",
    "        \n",
    "        print(\"Training Discriminator | Epoch:\", _epochs)\n",
    "        \n",
    "        # Train the discriminator for nth extra steps\n",
    "        for i in range(d_extra_steps):\n",
    "            # Real Image Discriminator Training\n",
    "            [X_real, real_labels], y_real = sample_real_samples(real_data, batch_size) # Note X_Real is [data, labels]\n",
    "            d_loss1 = d_model.train_on_batch([tf.stack(X_real), tf.stack(real_labels)], y_real) # Training on real\n",
    "\n",
    "            # Fake Image Discriminator Training\n",
    "            [X_fake, fake_labels], y_fake = generate_samples(g_model, latent_dim, batch_size)\n",
    "            d_loss2 = d_model.train_on_batch([tf.stack(X_fake), tf.stack(fake_labels)], y_fake) # Training on fakes\n",
    "            \n",
    "        # Create a latent space and inverted labels\n",
    "        noise_z, labels = latentDimensionalGenerator(latent_dim, batch_size) # Latent space generation\n",
    "        pretend_real = np.ones((batch_size, 1))\n",
    "        \n",
    "        print()\n",
    "        print(\"Training Generator | Epoch:\", _epochs)\n",
    "        \n",
    "        g_loss = gan_model.train_on_batch([tf.stack(noise_z), tf.stack(labels)], pretend_real)\n",
    "        \n",
    "        return {\"d_loss1\": d_loss1, \"d_loss2\": d_loss2, \"g_loss\": g_loss}\n",
    "    \n",
    "def produceSampleGraphs(g_model, latent_dim, n_samples):\n",
    "    # Produce 255 Sample graphs given the data\n",
    "    noise_z, labels = latentDimensionalGenerator(latent_dim, n_samples) # Latent space generation\n",
    "    generated_parameters = g_model.predict([noise_z, labels])\n",
    "    tensor_generated = generated_parameters\n",
    "    \n",
    "    for curve in tensor_generated:\n",
    "        plt.plot(feature_domain_2, curve);\n",
    "    plt.show(); \n",
    "    \n",
    "    return;\n",
    "\n",
    "\n",
    "# train the generator and discriminator\n",
    "def fit_wgan(g_model, d_model, gan_model, real_data, latent_dim, n_epochs, n_batch, save_path=\"\"):\n",
    "    d1Loss = [];\n",
    "    d2Loss = [];\n",
    "    gLoss = [];\n",
    "    \n",
    "    for _epochs in range(n_epochs):\n",
    "            losses = train_step(\n",
    "                g_model, \n",
    "                d_model, \n",
    "                gan_model, \n",
    "                real_data, \n",
    "                n_batch, \n",
    "                5, \n",
    "                latent_dim, \n",
    "                _epochs\n",
    "            )\n",
    "            print(losses)\n",
    "            print()\n",
    "            produceSampleGraphs();\n",
    "            \n",
    "    return;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41afcbd",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e39a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_discriminator = conditional_Wasserstein_Discriminator();\n",
    "w_generator = conditional_Wasserstein_Generator();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134bb18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_discriminator.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d8be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_generator.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755460e0",
   "metadata": {},
   "source": [
    "### WGAN-GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d033e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_wass = WGAN_GP(w_discriminator, w_generator) # Initalize model\n",
    "# print(model_wass)\n",
    "\n",
    "model_wass = define_wgan(w_discriminator, w_generator);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4c72a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_wass.compile(\n",
    "#     g_optimizer = generator_optimizer,\n",
    "#     g_loss_fn = g_wasserstein_loss\n",
    "# );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8dea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000;\n",
    "batch_size = 4;\n",
    "real_data = X;\n",
    "\n",
    "fit_wgan(\n",
    "    w_generator,\n",
    "    w_discriminator,\n",
    "    model_wass,\n",
    "    real_data\n",
    "    2,\n",
    "    n_epochs,\n",
    "    batch_size\n",
    ");\n",
    "\n",
    "# model_wass.fit(\n",
    "#     n_epochs = n_epochs,\n",
    "#     batch_size = batch_size,\n",
    "#     real_data = real_data\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "518eb2a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43931822",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9591de17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
