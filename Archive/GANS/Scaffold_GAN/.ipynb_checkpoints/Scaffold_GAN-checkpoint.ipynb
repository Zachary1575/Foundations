{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b297139a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regular Funcs\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image as ImagePIL\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from numpy.random import randint\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa618cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistics\n",
    "from scipy import stats\n",
    "from scipy import integrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca544e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-03 09:03:31.338868: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import Reshape\n",
    "from keras import backend\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.constraints import Constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7be73158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Funcs\n",
    "from Unpack_Scaffold_Data import readAndOutputDataset, curveVisualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29e58e2",
   "metadata": {},
   "source": [
    "# Data Read Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4957952",
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_path = \"/Users/zacharyg/Documents/GitHub/fundemental-neural-nets/GANS/Scaffold_GAN/scaffold_dataset_WU_LAB/Prints\"\n",
    "modulus_path = \"/Users/zacharyg/Documents/GitHub/fundemental-neural-nets/GANS/Scaffold_GAN/scaffold_dataset_WU_LAB/Prints/modulus_data_types.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25a2f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC COUNT: 675\n",
      "Operation Finished.\n",
      "\n",
      "     Index     Modulus  Spacing  Infill  Height  Speed  Temperature   Mass  \\\n",
      "0        1  358.528888      0.8       1     0.1     30          190  0.394   \n",
      "1        2  301.639039      0.9       1     0.1     30          190  0.334   \n",
      "2        3  292.501492      1.0       1     0.1     30          190  0.308   \n",
      "3        4  258.539802      1.1       1     0.1     30          190  0.286   \n",
      "4        5  238.213024      1.2       1     0.1     30          190  0.259   \n",
      "..     ...         ...      ...     ...     ...    ...          ...    ...   \n",
      "670    671  151.559731      0.8       3     0.2     50          230  0.428   \n",
      "671    672   85.074096      0.9       3     0.2     50          230  0.341   \n",
      "672    673   52.285252      1.0       3     0.2     50          230  0.290   \n",
      "673    674   70.811230      1.1       3     0.2     50          230  0.292   \n",
      "674    675   36.627466      1.2       3     0.2     50          230  0.244   \n",
      "\n",
      "     Porosity    Type  \n",
      "0      0.6848    Line  \n",
      "1      0.7328    Line  \n",
      "2      0.7536    Line  \n",
      "3      0.7712    Line  \n",
      "4      0.7928    Line  \n",
      "..        ...     ...  \n",
      "670    0.6576  Gyroid  \n",
      "671    0.7272  Gyroid  \n",
      "672    0.7680  Gyroid  \n",
      "673    0.7664  Gyroid  \n",
      "674    0.8048  Gyroid  \n",
      "\n",
      "[675 rows x 10 columns]\n",
      "Operation Finished.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X, y, y_df, file_order = readAndOutputDataset(curve_path, modulus_path, reverse=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a9f3d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X SHAPE: (675, 2, 1803)\n",
      "y SHAPE: (675, 10)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity Check\n",
    "print(\"X SHAPE:\", X.shape);\n",
    "print(\"y SHAPE:\", y.shape);\n",
    "print();\n",
    "\n",
    "\n",
    "# Visualization\n",
    "# curveVisualization(X, y, file_order);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5c987",
   "metadata": {},
   "source": [
    "# Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b72bd77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transposeStressData(X_Data):\n",
    "    X = [];\n",
    "    \n",
    "    for data in X_Data:\n",
    "        X.append(data.T);\n",
    "        \n",
    "    return np.array(X);\n",
    "\n",
    "def normalizeStressStrain(x):\n",
    "    for curve_index in range(len(x)):\n",
    "        curve = x[curve_index];\n",
    "        \n",
    "        max_stress_val = np.max(curve[0]);\n",
    "        max_strain_val = np.max(curve[1]);\n",
    "        \n",
    "        curve[0] = curve[0] / max_stress_val;\n",
    "        curve[1] = curve[1] / max_strain_val;\n",
    "        \n",
    "    return x;\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    \n",
    "    Parameters\n",
    "    -----------------\n",
    "    x: Array of Homogenous (RGB) values of input data \n",
    "    \n",
    "    Returns\n",
    "    -----------------\n",
    "    new_imgs: (numpy integer array) Numpy array of normalized data\n",
    "    \"\"\"\n",
    "    return np.array((x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "\n",
    "def stringtoCategorical(y):    \n",
    "    data = [];\n",
    "    \n",
    "    for type_index in range(len(y)):\n",
    "        wrd = y[type_index];\n",
    "        encoding = 0.0;\n",
    "        \n",
    "        if (wrd == \"Cubic\"):\n",
    "            encoding = 1.0;\n",
    "        elif (wrd == \"Gyroid\"):\n",
    "            encoding = 2.0;\n",
    "            \n",
    "        data.append([encoding]);\n",
    "        \n",
    "    return np.array(data);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387fc5f5",
   "metadata": {},
   "source": [
    "# Process Parameter Stripping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e158b51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameterStrip(y):\n",
    "    y_t = y.T;\n",
    "    \n",
    "    Index = y_t[0];\n",
    "    Modulus = y_t[1];\n",
    "    Spacing = y_t[2];\n",
    "    Infill = y_t[3];\n",
    "    Height = y_t[4];\n",
    "    Speed = y_t[5];\n",
    "    Temp = y_t[6];\n",
    "    Mass = y_t[7];\n",
    "    Porosity = y_t[8];\n",
    "    Type = y_t[9];\n",
    "    return Index, Modulus, Spacing, Infill, Height, Speed, Temp, Mass, Porosity, Type\n",
    "\n",
    "Index, Modulus, Spacing, Infill, Height, Speed, Temp, Mass, Porosity, Type = parameterStrip(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b7b9a7",
   "metadata": {},
   "source": [
    "# Energy Absorption Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b3c0d9",
   "metadata": {},
   "source": [
    "Taking the integration under the curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bb9dec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(675,)\n"
     ]
    }
   ],
   "source": [
    "Energy_Absorption = [];\n",
    "\n",
    "for curve in X:\n",
    "    interval_x = curve[0];\n",
    "    interval_y = curve[1];\n",
    "    \n",
    "    val = integrate.simpson(interval_y, interval_x);\n",
    "    Energy_Absorption.append(val);\n",
    "    \n",
    "Energy_Absorption = np.array(Energy_Absorption);\n",
    "\n",
    "# Sanity Check\n",
    "print(Energy_Absorption.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdda64b",
   "metadata": {},
   "source": [
    "# Plotting: EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85ed3ee",
   "metadata": {},
   "source": [
    "Here we take curve parameters and understand how the data is distributed and predicted. Understanding the data is half the problem!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bca131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 675 Stress-Strain Curve Domain\n",
    "feature_domain_675 = list(range(675 + 1));\n",
    "feature_domain_675.pop(0) \n",
    "feature_domain_675 = np.repeat(feature_domain_675, 4, axis=0) # Changed to 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0071b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_domain_8 = list(range(8 + 1));\n",
    "feature_domain_8.pop(0);\n",
    "feature_domain_8_rep = list(np.arange(1,9))*675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_domain_7 = list(range(7 + 1));\n",
    "feature_domain_7.pop(0);\n",
    "feature_domain_7_rep = list(np.arange(1,8)) * 675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69fa2ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_domain_4 = list(range(4 + 1));\n",
    "feature_domain_4.pop(0);\n",
    "feature_domain_4_rep = list(np.arange(1,5)) * 675"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648d82c",
   "metadata": {},
   "source": [
    "***NOTABLE FEATURES:***\n",
    "\n",
    "For Scatter: 230C printing temperature has a void of Modulus values from 400-500 modulus values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dab1bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modulus vs. Temperature\n",
    "# X = Temperature\n",
    "# Y = Modulus\n",
    "fig = px.scatter(\n",
    "    x=Temp, \n",
    "    y=Modulus,\n",
    "    title=\"Each point of dataset over 675 curves\",\n",
    "    labels = {\"x\": \"Temperature\", \"y\": \"Modulus\"}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a820aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modulus vs. Print Speed\n",
    "# X = Temperature\n",
    "# Y = Modulus\n",
    "fig = px.scatter(\n",
    "    x=Speed, \n",
    "    y=Modulus,\n",
    "    title=\"Each point of dataset over 675 curves\",\n",
    "    labels = {\"x\": \"Print Speed\", \"y\": \"Modulus\"}\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f54f8c4",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeccd38",
   "metadata": {},
   "source": [
    "Each Clustering is based on the different characteristics of the curve. \n",
    "\n",
    "Most plots are a porosity vs Compression Modulus Plot, we see each variable how influential it is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc617f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Parameter_Sum_Str = [];\n",
    "\n",
    "for index in range(len(Spacing)):\n",
    "    Summation = Spacing[index] + Infill[index] + Height[index] + Speed[index] + Temp[index];\n",
    "    Parameter_Sum_Str.append(str(Summation));\n",
    "    \n",
    "Parameter_Sum_Str = np.array(Parameter_Sum_Str);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "combination_str = [];\n",
    "\n",
    "for curve in y:\n",
    "    combination_str.append(str(curve));\n",
    "\n",
    "combination_str = np.array(combination_str);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74995295",
   "metadata": {},
   "outputs": [],
   "source": [
    "Modulus_N = Modulus/np.max(Modulus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a14aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=Porosity, \n",
    "    y=Modulus_N,\n",
    "    title=\"Porosity vs. Modulus\",\n",
    "    labels = {\"x\": \"Porosity\", \"y\": \"Modulus\"},\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee5a5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=Porosity, \n",
    "    y=Modulus,\n",
    "    title=\"Temperature | Porosity vs. Modulus\",\n",
    "    labels = {\"x\": \"Porosity\", \"y\": \"Modulus\"},\n",
    "    text=Temp\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a86d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=Porosity, \n",
    "    y=Modulus,\n",
    "    title=\"Speed | Porosity vs. Modulus\",\n",
    "    labels = {\"x\": \"Porosity\", \"y\": \"Modulus\"},\n",
    "    text=Speed\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a27c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=Porosity, \n",
    "    y=Modulus,\n",
    "    title=\"Type | Porosity vs. Modulus\",\n",
    "    labels = {\"x\": \"Porosity\", \"y\": \"Modulus\"},\n",
    "    text=Type\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e763d1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=Porosity, \n",
    "    y=Modulus,\n",
    "    title=\"Spacing | Porosity vs. Modulus\",\n",
    "    labels = {\"x\": \"Porosity\", \"y\": \"Modulus\"},\n",
    "    text=Spacing\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638398bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=Porosity, \n",
    "    y=Modulus,\n",
    "    title=\"Height | Porosity vs. Modulus\",\n",
    "    labels = {\"x\": \"Porosity\", \"y\": \"Modulus\"},\n",
    "    text=Height\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2809e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=Porosity, \n",
    "    y=Modulus,\n",
    "    title=\"ALL | Porosity vs. Modulus\",\n",
    "    labels = {\"x\": \"Porosity\", \"y\": \"Modulus\"},\n",
    "    text=combination_str\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a07abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    x=Porosity, \n",
    "    y=Modulus,\n",
    "    title=\"Parameter Sum | Porosity vs. Modulus\",\n",
    "    labels = {\"x\": \"Porosity\", \"y\": \"Modulus\"},\n",
    "    text=Parameter_Sum_Str\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c0fbff",
   "metadata": {},
   "source": [
    "### 3D Visualization of Modulus vs. Porosity vs. Energy Absorption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39986e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = px.data.iris()\n",
    "\n",
    "fig = px.scatter_3d(\n",
    "    x=Porosity, \n",
    "    y=Modulus, \n",
    "    z=Energy_Absorption, \n",
    "    color=Type\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72cc677",
   "metadata": {},
   "source": [
    "# Combination Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed8875",
   "metadata": {},
   "source": [
    "The hard part of the problem is encapsulating the combination of variables to generate different types of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4721a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_params_a = y[:, 1:2];\n",
    "cut_params_b = y[:, 5:7];\n",
    "cut_params_c = y[:, 8:9];\n",
    "cut_params_combo = np.concatenate((cut_params_a, cut_params_b, cut_params_c), axis=1);\n",
    "\n",
    "# Scale up to 10- 1000\n",
    "for arr in cut_params_combo:\n",
    "    arr[1] = arr[1] * 10\n",
    "    arr[3] = arr[3] * 1000\n",
    "    \n",
    "print(cut_params_combo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ec6b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03abcda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d754f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab65b4ac",
   "metadata": {},
   "source": [
    "# Scaffold-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d09733",
   "metadata": {},
   "source": [
    "### Dataset Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e94d7a",
   "metadata": {},
   "source": [
    "Let $D$ be a set of sets such that $|D| = 675$. Let $d_1, d_2, ..., d_n \\in D$ be the set within sets. Printing scalar values that are set during the printing process. Suppose that $C$ and $P$ denote curve parameters and printing parameters respectively.\n",
    "\n",
    "We noticed that infill type has the highest ***Spearman Correlation*** and ***Man Whitney U Sig***. We denote that for each $d_n$, there is a set of curve parameters $C_n$ and a set of printing parameters  $P_n$. We denote that $C_n = \\{c_1, c_2, c_3\\}$ where the parameters respectively represent ***Compression Modulus, Porosity, and Energy Absorption***. We denote that $P_n = \\{p_1, p_2, p_3, p_4\\}$ where the parameters respectively represent ***Spacing, Height, Speed, Temperature***. Note that $C_n, P_n \\in d_n$.\n",
    "\n",
    "An additional parameter to note, for **Infill Type**, the values are fixed discretely such that:\n",
    "\n",
    " $$Infill \\ Type(P_2) = \\begin{cases}\n",
    "  0  & P_2 \\text{ is Line} \\\\\n",
    "  1  & P_2 \\text{ is Cubic} \\\\\n",
    "  2  & P_2 \\text{ is Gyroid}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78cea56",
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_params_1 = y[:, 1:2];\n",
    "cut_params_2 = y[:, 8:9];\n",
    "\n",
    "C_n = np.concatenate((cut_params_1, cut_params_2, (np.reshape(Energy_Absorption, (675,1)))), axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4398a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(C_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166dbc0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28edecae",
   "metadata": {},
   "source": [
    "# Classification Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7693675c",
   "metadata": {},
   "source": [
    "### Data Preparation & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772a79ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encoding(y):\n",
    "    integer_encoding = [];\n",
    "    \n",
    "    for i in y:\n",
    "        integer_encoding.append(i - 1);\n",
    "    \n",
    "    return np.array(integer_encoding);\n",
    "\n",
    "y_label_categorical = integer_encoding(Infill);\n",
    "\n",
    "y_label_categorical = tf.keras.utils.to_categorical(\n",
    "    y_label_categorical, num_classes=3, dtype='float32'\n",
    ");\n",
    "\n",
    "print(y_label_categorical.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb4d7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(C_n, y_label_categorical, test_size=0.1, random_state=1)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "\n",
    "print(\"X_train Shape:\", X_train.shape);\n",
    "print(\"y_train Shape:\", y_train.shape);\n",
    "print(\"X_val Shape:\", X_val.shape);\n",
    "print(\"y_val Shape:\", y_val.shape);\n",
    "print(\"X_test Shape:\", X_test.shape);\n",
    "print(\"y_test Shape:\", y_test.shape);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4e6199",
   "metadata": {},
   "source": [
    "### FC Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450c463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def InfillClassifier(in_shape = 3):\n",
    "    in_parameters = tf.keras.Input(shape=in_shape)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(10, input_dim=in_shape, activation='relu')(in_parameters)\n",
    "    x = tf.keras.layers.Dense(5, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(3, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(3, activation='relu')(x)\n",
    "    out = tf.keras.layers.Dense(3, activation='softmax')(x) # Output layer\n",
    "    \n",
    "    model = tf.keras.Model(in_parameters, out)\n",
    "    \n",
    "    opt = Adam(learning_rate =0.0001)\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy', \n",
    "        optimizer = opt, \n",
    "        metrics=['accuracy']\n",
    "    );\n",
    "    \n",
    "    return model;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e5b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "infillModel = InfillClassifier();\n",
    "\n",
    "infillModel.summary();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b87c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 3000\n",
    "X_train = X_train.astype('float32') # Convert to float32\n",
    "X_val = X_val.astype('float32') # Convert to float32\n",
    "\n",
    "history = infillModel.fit(\n",
    "    X_train, y_train, batch_size=4, epochs=n_epochs, validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1a7bbb",
   "metadata": {},
   "source": [
    "### Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_domain = list(range(n_epochs + 1))\n",
    "epochs_domain.pop(0)\n",
    "\n",
    "\n",
    "#  \"Accuracy\"\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()\n",
    "# \"Loss\"\n",
    "plt.plot()\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48080753",
   "metadata": {},
   "source": [
    "### Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb816d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.astype('float32') # Convert to float32\n",
    "predict = infillModel.predict(X_test);\n",
    "\n",
    "print(\"Samples:\", len(X_test))\n",
    "print()\n",
    "\n",
    "incorrect_count = 0;\n",
    "\n",
    "for predicted_index in range(len(predict)):\n",
    "    arr = predict[predicted_index]\n",
    "    truth = y_test[predicted_index]\n",
    "    arr[arr < np.max(arr)] = 0\n",
    "    arr[arr == np.max(arr)] = 1\n",
    "    \n",
    "    if (not np.array_equal(arr, truth)):\n",
    "        incorrect_count += 1;\n",
    "        \n",
    "        print(\"Incorrectly predicted!\")\n",
    "        print(\"Expected:\", truth)\n",
    "        print(\"Predicted:\", arr)\n",
    "        print();\n",
    "        \n",
    "print(\"Incorrect:\", str(int((incorrect_count / len(predict)) * 100)) + \"%\")\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c92c5d4",
   "metadata": {},
   "source": [
    "# Individualized GAN's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3acf46",
   "metadata": {},
   "source": [
    "Now we see that given the infill type that best fits the stress-strain curve, what are the optimal parameters?\n",
    "\n",
    "Recall that we denote that $P_n = \\{p_1, p_2, p_3, p_4\\}$ where the parameters respectively represent ***Spacing, Height, Speed, Temperature*** and $P_n \\in d_n$. \n",
    "\n",
    "However, we want to  $C_n \\cup P_n$. Thus, we apply each data to the GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1539d0e",
   "metadata": {},
   "source": [
    "### Seperate Data by Infill Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dccc970",
   "metadata": {},
   "outputs": [],
   "source": [
    "Line_Data = [];\n",
    "Cubic_Data = [];\n",
    "Gyroid_Data = [];\n",
    "\n",
    "_y = cut_params = np.concatenate((\n",
    "    y,\n",
    "    (np.reshape(Energy_Absorption, (675,1))),\n",
    "), axis=1);\n",
    "\n",
    "for curve in _y:\n",
    "    if ('Gyroid' in curve):\n",
    "        Gyroid_Data.append(curve);\n",
    "    elif ('Cubic' in curve):\n",
    "        Cubic_Data.append(curve);\n",
    "    elif ('Line' in curve):\n",
    "        Line_Data.append(curve);\n",
    "        \n",
    "Line_Data = np.array(Line_Data);\n",
    "Cubic_Data = np.array(Cubic_Data);\n",
    "Gyroid_Data = np.array(Gyroid_Data);\n",
    "      \n",
    "# Sanity Check\n",
    "print(Line_Data.shape)\n",
    "print(Cubic_Data.shape)\n",
    "print(Gyroid_Data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d9f933",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faff3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organizeParameters(_Data):\n",
    "    Modulus = _Data[:, 1:2];\n",
    "    Porosity = _Data[:, 8:9];\n",
    "    Energy_Abs = _Data[:, 10:11];\n",
    "    Spacing = _Data[:, 2:3];\n",
    "    printing_params = _Data[:, 4:7];\n",
    "\n",
    "    cut_params = np.concatenate((\n",
    "        Modulus, \n",
    "        Porosity,\n",
    "        Energy_Abs,\n",
    "        Spacing,\n",
    "        printing_params\n",
    "    ), axis=1);\n",
    "    \n",
    "    return cut_params;\n",
    "\n",
    "X_Line = organizeParameters(Line_Data);\n",
    "X_Cubic = organizeParameters(Cubic_Data);\n",
    "X_Gyroid = organizeParameters(Gyroid_Data);\n",
    "\n",
    "# Sanity Check\n",
    "print(X_Line.shape)\n",
    "print(X_Cubic.shape)\n",
    "print(X_Gyroid.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c45bc8",
   "metadata": {},
   "source": [
    "### Plot Curves for some Visualization:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6828d227",
   "metadata": {},
   "source": [
    "Note that it goes by: \n",
    "\n",
    "$$d_n =  \\{ \\text{Modulus}, \\text{Porosity}, \\text{Energy Absorption}, \\text{Height}, \\text{Distance}, \\text{Speed}, \\text{Temp} \\} $$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f75a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single Lines Chart (DISTRIBUTION)\n",
    "# fig_k = px.line(\n",
    "#     x=feature_domain_7, \n",
    "#     y=X_Line[200],\n",
    "#     title=\"Line: Single Parameter Curve\",\n",
    "#     labels={\"x\": \"Parameters\", \"y\":\"Unnormalized values\"}\n",
    "# )\n",
    "\n",
    "# fig_k.show()\n",
    "\n",
    "\n",
    "# # Multiple Lines Chart (DISTRIBUTION)\n",
    "# fig = go.Figure()\n",
    "\n",
    "# for line in range(len(X_Line)):\n",
    "#     data = X_Line[line];\n",
    "#     fig.add_trace(go.Scatter(x=feature_domain_7, y=data))\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eb93e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Single Lines Chart (DISTRIBUTION)\n",
    "# fig_k = px.line(\n",
    "#     x=feature_domain_7, \n",
    "#     y=X_Cubic[200],\n",
    "#     title=\"Cubic: Single Parameter Curve\",\n",
    "#     labels={\"x\": \"Parameters\", \"y\":\"Unnormalized values\"}\n",
    "# )\n",
    "\n",
    "# fig_k.show()\n",
    "\n",
    "\n",
    "# # Multiple Lines Chart (DISTRIBUTION)\n",
    "# fig = go.Figure()\n",
    "\n",
    "# for line in range(len(X_Line)):\n",
    "#     data = X_Cubic[line];\n",
    "#     fig.add_trace(go.Scatter(x=feature_domain_7, y=data))\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e9ec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Single Lines Chart (DISTRIBUTION)\n",
    "# fig_k = px.line(\n",
    "#     x=feature_domain_7, \n",
    "#     y=X_Gyroid[200],\n",
    "#     title=\"Cubic: Single Parameter Curve\",\n",
    "#     labels={\"x\": \"Parameters\", \"y\":\"Unnormalized values\"}\n",
    "# )\n",
    "\n",
    "# fig_k.show()\n",
    "\n",
    "\n",
    "# # Multiple Lines Chart (DISTRIBUTION)\n",
    "# fig = go.Figure()\n",
    "\n",
    "# for line in range(len(X_Line)):\n",
    "#     data = X_Gyroid[line];\n",
    "#     fig.add_trace(go.Scatter(x=feature_domain_7, y=data))\n",
    "\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0040303d",
   "metadata": {},
   "source": [
    "## Mode Specific Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029ccb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00db2368",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecf400f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7b7599",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"X_Line Max:\", np.max(X_Line));\n",
    "X_Line_N = X_Line / np.max(X_Line);\n",
    "\n",
    "print(\"X_Cubic Max:\", np.max(X_Cubic));\n",
    "X_Cubic_N = X_Cubic / np.max(X_Cubic);\n",
    "\n",
    "print(\"X_Gyroid Max:\", np.max(X_Gyroid));\n",
    "X_Gyroid_N = X_Gyroid / np.max(X_Gyroid);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee6980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Lines Chart (DISTRIBUTION)\n",
    "fig_k = px.line(\n",
    "    x=feature_domain_7, \n",
    "    y=X_Line_N[200],\n",
    "    title=\"Line: Single Parameter Curve\",\n",
    "    labels={\"x\": \"Parameters\", \"y\":\"Unnormalized values\"}\n",
    ")\n",
    "\n",
    "fig_k.show()\n",
    "\n",
    "\n",
    "# Multiple Lines Chart (DISTRIBUTION)\n",
    "fig = go.Figure()\n",
    "\n",
    "for line in range(len(X_Line_N)):\n",
    "    data = X_Line_N[line];\n",
    "    fig.add_trace(go.Scatter(x=feature_domain_7, y=data))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd43ac4",
   "metadata": {},
   "source": [
    "# Simple GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1da497",
   "metadata": {},
   "source": [
    "### Discriminator Data Sampling Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040d57a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_real_samples(dataset, n_samples):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    --------------\n",
    "    real_dataset: dataset with the real data\n",
    "    n_samples: amount of real images to sample from\n",
    "    \n",
    "    Returns\n",
    "    --------------\n",
    "    X: samples of n images in a list\n",
    "    Y: labels of (1's) for true images (Binary Classification)\n",
    "    \"\"\"\n",
    "    if (isinstance(dataset, list)):\n",
    "        dataset = np.asarray(dataset);\n",
    "        \n",
    "    random_num = randint(0, dataset.shape[0], n_samples);\n",
    "    X = dataset[random_num];\n",
    "    y = np.ones((n_samples, 1));\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539a6d56",
   "metadata": {},
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9f8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleDiscriminator(in_shape=7):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential();\n",
    "    \n",
    "    model.add(Dense(100, input_dim=in_shape, activation='relu')) \n",
    "    model.add(Dense(7, input_dim=in_shape, activation='relu')) \n",
    "    model.add(Dense(1, activation='sigmoid')) # Since the decision is binary (Real | Fake), we use sigmoid\n",
    "    \n",
    "    opt = Adam(learning_rate =0.001)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy', \n",
    "        optimizer = opt, \n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694fd980",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a53b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleGenerator(in_shape=7):\n",
    "    model = tf.keras.Sequential();\n",
    "    \n",
    "    model.add(Dense(7, input_dim=in_shape, activation=\"relu\"))\n",
    "    model.add(Dense(7, input_dim=in_shape, activation=\"relu\"))\n",
    "    model.add(Dense(7)) \n",
    "    \n",
    "    return model;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d14b7e9",
   "metadata": {},
   "source": [
    "### Summary of Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3c82af",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = simpleDiscriminator();\n",
    "generator = simpleGenerator();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4e3a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.summary();\n",
    "generator.summary();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4529f9e",
   "metadata": {},
   "source": [
    "### Latent Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c644b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latentDimensionalGenerator(latent_dimensions, n_samples, randomGaussian = False):\n",
    "    data = [];\n",
    "    \n",
    "    for sample in range(n_samples):\n",
    "        x_input_0 = np.random.randn(latent_dimensions); # Points sampled from a normalized distribution.\n",
    "        data.append(x_input_0);\n",
    "        \n",
    "    return np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2030f303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator production\n",
    "def generate_samples(g_model, latent_dim, n_samples):\n",
    "    x_input = latentDimensionalGenerator(latent_dim, n_samples)  # generate points in a latent space\n",
    "    X = g_model.predict(x_input)\n",
    "    y = np.zeros((n_samples, 1))  # create 'fake' class labels (0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf5bb6",
   "metadata": {},
   "source": [
    "### Visualizing the latent dimensional space in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k = latentDimensionalGenerator(7, 10)\n",
    "# print(k)\n",
    "\n",
    "fake_X, fake_y = generate_samples(generator, 7, 10);\n",
    "print(fake_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebc66b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for curve in fake_X:\n",
    "    fig = go.Figure();\n",
    "    fig.add_trace(go.Scatter(x=feature_domain_7, y=curve));\n",
    "    fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ef5555",
   "metadata": {},
   "source": [
    "### GAN: Putting it together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfba3d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_gan(generator, discriminator):\n",
    "    discriminator.trainable = False # We set the discriminator as not trainable so the generator updates\n",
    "    model = tf.keras.Sequential() \n",
    "    \n",
    "    model.add(generator)\n",
    "    model.add(discriminator)\n",
    "    \n",
    "    opt = Adam(learning_rate = 0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt) # Generator will train on this loss\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddd3b6d",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10a47c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples, save_path=\"\"):\n",
    "    # Real Images based on discriminator\n",
    "    X_real, y_real = sample_real_samples(dataset, n_samples)\n",
    "    _, acc_real = d_model.evaluate(X_real, y_real, verbose=0)\n",
    "    \n",
    "    # Fake Images based on discriminator\n",
    "    x_fake, y_fake = generate_samples(g_model, latent_dim, n_samples)\n",
    "    _, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)\n",
    "    \n",
    "    print(\"============== CURVE GENERATION ON EPOCH\", epoch,\"==============\");\n",
    "    \n",
    "    for curve in x_fake:\n",
    "        plt.plot(feature_domain_7, curve)\n",
    "    \n",
    "    if (save_path != \"\"):\n",
    "        plt.title(\"Training in epoch: \" + str(epoch))\n",
    "        plt.savefig(os.path.join(save_path, str(epoch) + '.png'));\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3790d798",
   "metadata": {},
   "source": [
    "# GAN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7347ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train_gan(g_model, d_model, gan_model, training_data, latent_dim, n_epochs, n_batch, save_path=\"\"):\n",
    "    d1Loss = [];\n",
    "    d2Loss = [];\n",
    "    gLoss = [];\n",
    "    \n",
    "    half_batch = int(n_batch / 2);\n",
    "    \n",
    "    for i in range(n_epochs):                \n",
    "        # Real Image Discriminator Training\n",
    "        X_real, y_real = sample_real_samples(training_data, half_batch)\n",
    "        d_loss1, _ = d_model.train_on_batch(X_real, y_real) # Training on real\n",
    "\n",
    "        # Fake Image Discriminator Training\n",
    "        X_fake, y_fake = generate_samples(g_model, latent_dim, half_batch)\n",
    "        d_loss2, _ = d_model.train_on_batch(X_fake, y_fake) # Training on fakes\n",
    "\n",
    "        # Create a latent space and inverted labels\n",
    "        X_gan = latentDimensionalGenerator(latent_dim, n_batch)\n",
    "        y_gan = np.ones((n_batch, 1)) # Pretend that that they are all real.\n",
    "\n",
    "        # Update the generator via the discriminator's error\n",
    "        g_loss = gan_model.train_on_batch(X_gan, y_gan)\n",
    "\n",
    "        # summarize loss on this batch\n",
    "        print('>%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, d_loss1, d_loss2, g_loss))\n",
    "        summarize_performance(i, g_model, d_model, training_data, latent_dim, 100, save_path)\n",
    "        \n",
    "        d1Loss.append(d_loss1);\n",
    "        d2Loss.append(d_loss2);\n",
    "        gLoss.append(g_loss);\n",
    "        \n",
    "    return d1Loss, d2Loss, gLoss;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053ba198",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 7;\n",
    "gan_model = define_gan(generator, discriminator);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(os.getcwd());\n",
    "# os.chdir(\"/Users/zacharyg/Documents/GitHub/fundemental-neural-nets/GANS/Scaffold_GAN\");\n",
    "# image_save_path = \"./images/\"\n",
    "\n",
    "# if not os.path.exists(image_save_path):\n",
    "#     os.makedirs(image_save_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1734a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 2000;\n",
    "X_Line_N = X_Line_N.astype('float32')\n",
    "\n",
    "#Training\n",
    "d1, d2, gloss = train_gan(\n",
    "    generator, \n",
    "    discriminator, \n",
    "    gan_model, \n",
    "    X_Line_N, \n",
    "    latent_dim, \n",
    "    n_epochs, # n_epochs\n",
    "    10,  # batch size\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a57c0fc",
   "metadata": {},
   "source": [
    "# Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05873ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e979eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b6523c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fd0587",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
